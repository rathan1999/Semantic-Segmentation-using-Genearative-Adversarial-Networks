{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_map(N=256, normalized=False):\n",
    "    def bitget(byteval, idx):\n",
    "        return ((byteval & (1 << idx)) != 0)\n",
    "\n",
    "    dtype = 'float32' if normalized else 'uint8'\n",
    "    cmap = np.zeros((N, 3), dtype=dtype)\n",
    "    for i in range(N):\n",
    "        r = g = b = 0\n",
    "        c = i\n",
    "        for j in range(8):\n",
    "            r = r | (bitget(c, 0) << 7-j)\n",
    "            g = g | (bitget(c, 1) << 7-j)\n",
    "            b = b | (bitget(c, 2) << 7-j)\n",
    "            c = c >> 3\n",
    "\n",
    "        cmap[i] = np.array([r, g, b])\n",
    "\n",
    "    cmap = cmap/255 if normalized else cmap\n",
    "    return cmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelToCode():\n",
    "    labels = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'void']\n",
    "    nclasses = 21\n",
    "    d={}\n",
    "    cmap = color_map()\n",
    "    for i in range(nclasses):\n",
    "        d[tuple(cmap[i].tolist())]=[labels[i],i+1]\n",
    "    d[tuple(cmap[-1].tolist())]=[labels[-1],nclasses+1]\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeImage(image):\n",
    "    resizedImage=np.zeros((224,224,3),dtype=np.int32)\n",
    "    if(image.shape[0]>224):\n",
    "        xDiff=image.shape[0]-224\n",
    "        if(image.shape[1]>224):\n",
    "            yDiff=image.shape[1]-224\n",
    "            resizedImage[:,:,:]=image[(xDiff>>1):(xDiff>>1)+224,(yDiff>>1):(yDiff>>1)+224,:]\n",
    "        else:\n",
    "            yDiff=224-image.shape[1]\n",
    "            resizedImage[:,(yDiff>>1):(yDiff>>1)+image.shape[1],:]=image[(xDiff>>1):(xDiff>>1)+224,:,:]\n",
    "    else:\n",
    "        xDiff=224-image.shape[0]\n",
    "        if(image.shape[1]>224):\n",
    "            yDiff=image.shape[1]-224\n",
    "            resizedImage[(xDiff>>1):(xDiff>>1)+image.shape[0],:,:]=image[:,(yDiff>>1):(yDiff>>1)+224,:]\n",
    "        else:\n",
    "            yDiff=224-image.shape[1]\n",
    "            resizedImage[(xDiff>>1):(xDiff>>1)+image.shape[0],(yDiff>>1):(yDiff>>1)+image.shape[1],:]=image[:,:,:]\n",
    "    return resizedImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageToMatrix(path,labels):\n",
    "    image=cv2.imread(path)\n",
    "    image=resizeImage(image)\n",
    "    a=np.zeros((image.shape[0],image.shape[1],23),dtype=np.int32)\n",
    "    for i in labels:\n",
    "        temp=np.where(np.all(image == i[::-1], axis=-1))\n",
    "        ind=(temp[0],temp[1],np.array(labels[i][-1]))\n",
    "        a[ind]=1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageToMatrix(\"E:\\\\DIP\\\\VOCtrain\\\\VOCdevkit\\\\VOC2012\\\\SegmentationClass\\\\2007_004468.png\",labelToCode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(Images=None,reuse=False,sess=None):\n",
    "    #Discriminator\n",
    "    Parameters=[]\n",
    "    with tf.variable_scope(tf.contrib.framework.get_name_scope(), reuse=reuse):\n",
    "        #Placeholder\n",
    "        if Images==None:\n",
    "            Images=tf.placeholder(tf.float32,shape=(None,224,224,3),name='Dinput')\n",
    "            vggmean=tf.constant([123.68,116.779,103.939], dtype=tf.float32, shape=[1,1,1,3])\n",
    "            Images=(Images-vggmean)\n",
    "        #Conv1\n",
    "        W1=tf.get_variable(\"DW1\",[3,3,3,64],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b1=tf.get_variable(\"Db1\",[64],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv1=tf.nn.conv2d(Images,W1,[1,1,1,1],padding='SAME',name='Dconv1')\n",
    "        z1=tf.nn.bias_add(conv1,b1,name='Dz1')\n",
    "        a1=tf.nn.relu(z1,name='Da1')\n",
    "        Parameters+=[W1,b1]\n",
    "        #Conv2\n",
    "        W2=tf.get_variable(\"DW2\",[3,3,64,64],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b2=tf.get_variable(\"Db2\",[64],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv2=tf.nn.conv2d(a1,W2,[1,1,1,1],padding='SAME',name='Dconv2')\n",
    "        z2=tf.nn.bias_add(conv2,b2,name='Dz2')\n",
    "        a2=tf.nn.relu(z2,name='Da2')\n",
    "        Parameters+=[W2,b2]\n",
    "        #Pool1\n",
    "        pool1=tf.nn.max_pool(a2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='Dpool1')\n",
    "        #Conv3\n",
    "        W3=tf.get_variable(\"DW3\",[3,3,64,128],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b3=tf.get_variable(\"Db3\",[128],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv3=tf.nn.conv2d(pool1,W3,[1,1,1,1],padding='SAME',name='Dconv3')\n",
    "        z3=tf.nn.bias_add(conv3,b3,name='Dz3')\n",
    "        a3=tf.nn.relu(z3,name='Da3')\n",
    "        Parameters+=[W3,b3]\n",
    "        #Conv4\n",
    "        W4=tf.get_variable(\"DW4\",[3,3,128,128],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b4=tf.get_variable(\"Db4\",[128],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv4=tf.nn.conv2d(a3,W4,[1,1,1,1],padding='SAME',name='Dconv4')\n",
    "        z4=tf.nn.bias_add(conv4,b4,name='Dz4')\n",
    "        a4=tf.nn.relu(z4,name='Da4')\n",
    "        Parameters+=[W4,b4]\n",
    "        #Pool2\n",
    "        pool2=tf.nn.max_pool(a4,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='Dpool2')\n",
    "        #Conv5\n",
    "        W5=tf.get_variable(\"DW5\",[3,3,128,256],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b5=tf.get_variable(\"Db5\",[256],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv5=tf.nn.conv2d(pool2,W5,[1,1,1,1],padding='SAME',name='Dconv5')\n",
    "        z5=tf.nn.bias_add(conv5,b5,name='Dz5')\n",
    "        a5=tf.nn.relu(z5,name='Da5')\n",
    "        Parameters+=[W5,b5]\n",
    "        #Conv6\n",
    "        W6=tf.get_variable(\"DW6\",[3,3,256,256],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b6=tf.get_variable(\"Db6\",[256],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv6=tf.nn.conv2d(a5,W6,[1,1,1,1],padding='SAME',name='Dconv6')\n",
    "        z6=tf.nn.bias_add(conv6,b6,name='Dz6')\n",
    "        a6=tf.nn.relu(z6,name='Da6')\n",
    "        Parameters+=[W6,b6]\n",
    "        #Conv7\n",
    "        W7=tf.get_variable(\"DW7\",[3,3,256,256],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b7=tf.get_variable(\"Db7\",[256],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv7=tf.nn.conv2d(a6,W7,[1,1,1,1],padding='SAME',name='Dconv7')\n",
    "        z7=tf.nn.bias_add(conv7,b7,name='Dz7')\n",
    "        a7=tf.nn.relu(z7,name='Da7')\n",
    "        Parameters+=[W7,b7]\n",
    "        #Pool3\n",
    "        pool3=tf.nn.max_pool(a7,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\",name='Dpool3')\n",
    "        #Conv8\n",
    "        W8=tf.get_variable(\"DW8\",[3,3,256,512],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b8=tf.get_variable(\"Db8\",[512],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv8=tf.nn.conv2d(pool3,W8,[1,1,1,1],padding='SAME',name='Dconv8')\n",
    "        z8=tf.nn.bias_add(conv8,b8,name='Dz8')\n",
    "        a8=tf.nn.relu(z8,name='Da8')\n",
    "        Parameters+=[W8,b8]\n",
    "        #Conv9\n",
    "        W9=tf.get_variable(\"DW9\",[3,3,512,512],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b9=tf.get_variable(\"Db9\",[512],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv9=tf.nn.conv2d(a8,W9,[1,1,1,1],padding='SAME',name='Dconv9')\n",
    "        z9=tf.nn.bias_add(conv9,b9,name='Dz9')\n",
    "        a9=tf.nn.relu(z9,name='Da9')\n",
    "        Parameters+=[W9,b9]\n",
    "        #Conv10\n",
    "        W10=tf.get_variable(\"DW10\",[3,3,512,512],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b10=tf.get_variable(\"Db10\",[512],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv10=tf.nn.conv2d(a9,W10,[1,1,1,1],padding='SAME',name='Dconv10')\n",
    "        z10=tf.nn.bias_add(conv10,b10,name='Dz10')\n",
    "        a10=tf.nn.relu(z10,name='Da10')\n",
    "        Parameters+=[W10,b10]\n",
    "        #Pool4\n",
    "        pool4=tf.nn.max_pool(a10,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='Dpool4')\n",
    "        #Conv11\n",
    "        W11=tf.get_variable(\"DW11\",[3,3,512,512],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b11=tf.get_variable(\"Db11\",[512],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv11=tf.nn.conv2d(pool4,W11,[1,1,1,1],padding='SAME',name='Dconv11')\n",
    "        z11=tf.nn.bias_add(conv11,b11,name='Dz11')\n",
    "        a11=tf.nn.relu(z11,name='Da11')\n",
    "        Parameters+=[W11,b11]\n",
    "        #Conv12\n",
    "        W12=tf.get_variable(\"DW12\",[3,3,512,512],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b12=tf.get_variable(\"Db12\",[512],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv12=tf.nn.conv2d(a11,W12,[1,1,1,1],padding='SAME',name='Dconv12')\n",
    "        z12=tf.nn.bias_add(conv12,b12,name='Dz12')\n",
    "        a12=tf.nn.relu(z12,name='Da12')\n",
    "        Parameters+=[W12,b12]\n",
    "        #Conv13\n",
    "        W13=tf.get_variable(\"DW13\",[3,3,512,512],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32,trainable=False)\n",
    "        b13=tf.get_variable(\"Db13\",[512],initializer=tf.constant_initializer(0.0),dtype=tf.float32,trainable=False)\n",
    "        conv13=tf.nn.conv2d(a12,W13,[1,1,1,1],padding='SAME',name='Dconv13')\n",
    "        z13=tf.nn.bias_add(conv13,b13,name='Dz13')\n",
    "        a13=tf.nn.relu(z13,name='Da13')\n",
    "        Parameters+=[W13,b13]\n",
    "        #Pool5\n",
    "        pool5=tf.nn.max_pool(a13,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='Dpool5')\n",
    "        #Deconv1\n",
    "        W14=tf.get_variable(\"DW14\",[3,3,128,512],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        b14=tf.get_variable(\"Db14\",[128],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        outputshape1=[tf.shape(pool5)[0],28,28,128]\n",
    "        deconv1=tf.nn.conv2d_transpose(pool5,W14,output_shape=outputshape1,strides=[1,4,4,1],padding='SAME',name='Dconv14')\n",
    "        z14=tf.nn.bias_add(deconv1,b14,name='Dz14')\n",
    "        a14=tf.nn.relu(z14,name='Da14')\n",
    "        Parameters+=[W14,b14]\n",
    "        #Deconv2\n",
    "        W15=tf.get_variable(\"DW15\",[3,3,32,128],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        b15=tf.get_variable(\"Db15\",[32],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        outputshape2=[tf.shape(a14)[0],112,112,32]\n",
    "        deconv2=tf.nn.conv2d_transpose(a14,W15,output_shape=outputshape2,strides=[1,4,4,1],padding='SAME',name='Dconv15')\n",
    "        z15=tf.nn.bias_add(deconv2,b15,name='Dz15')\n",
    "        a15=tf.nn.relu(z15,name='Da15')\n",
    "        Parameters+=[W15,b15]\n",
    "        #Deconv3\n",
    "        W16=tf.get_variable(\"DW16\",[3,3,23,32],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        b16=tf.get_variable(\"Db16\",[23],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        outputshape3=[tf.shape(a15)[0],224,224,23]\n",
    "        deconv3=tf.nn.conv2d_transpose(a15,W16,output_shape=outputshape3,strides=[1,2,2,1],padding='SAME',name='Dconv16')\n",
    "        z16=tf.nn.bias_add(deconv3,b16,name='Dz16')\n",
    "        Parameters+=[W16,b16]\n",
    "        \n",
    "    return z16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z=None,reuse=False):\n",
    "    if not(z):\n",
    "        z=tf.placeholder(tf.float32,shape=(None,100),name='GInput')\n",
    "    with tf.variable_scope(tf.contrib.framework.get_name_scope(), reuse=reuse):\n",
    "        #Linear\n",
    "        GW1=tf.get_variable(\"GW1\",[100,7*7*769],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gb1=tf.get_variable(\"Gb1\",[7*7*769],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        GprodTemp=tf.matmul(z,GW1,name=\"GprodTemp\")\n",
    "        Gprod=tf.nn.bias_add(GprodTemp,Gb1,name=\"Gprod\")\n",
    "        Gz1=tf.reshape(Gprod,[-1,7,7,769],name=\"Gz1\")\n",
    "        Ga1=tf.nn.relu(Gz1,name='Ga1')\n",
    "        #Deconv1\n",
    "        GW2=tf.get_variable(\"GW2\",[3,3,384,769],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gb2=tf.get_variable(\"Gb2\",[384],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        Goutshape1=[tf.shape(Ga1)[0],14,14,384]\n",
    "        Gdeconv1=tf.nn.conv2d_transpose(Ga1,GW2,output_shape=Goutshape1,strides=[1,2,2,1],padding=\"SAME\",name=\"Gdeconv1\")\n",
    "        Gz2=tf.nn.bias_add(Gdeconv1,Gb2,name='Gz2')\n",
    "        Ggamma1=tf.get_variable(\"Ggamma1\",[1],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gbeta1=tf.get_variable(\"Gbeta1\",[1],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gmean1,Gvar1=tf.nn.moments(Gz2,axes=[0],keep_dims=True)\n",
    "        Gbatch1=tf.nn.batch_normalization(Gz2,Gmean1,Gvar1,Gbeta1,Ggamma1,1e-5,name='Gbatch1')\n",
    "        Ga2=tf.nn.relu(Gbatch1,name='Ga2')\n",
    "        #Deconv2\n",
    "        GW3=tf.get_variable(\"GW3\",[3,3,256,384],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gb3=tf.get_variable(\"Gb3\",[256],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        Goutshape2=[tf.shape(Ga2)[0],28,28,256]\n",
    "        Gdeconv2=tf.nn.conv2d_transpose(Ga2,GW3,output_shape=Goutshape2,strides=[1,2,2,1],padding=\"SAME\",name=\"Gdeconv2\")\n",
    "        Gz3=tf.nn.bias_add(Gdeconv2,Gb3,name='Gz3')\n",
    "        Ggamma2=tf.get_variable(\"Ggamma2\",[1],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gbeta2=tf.get_variable(\"Gbeta2\",[1],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gmean2,Gvar2=tf.nn.moments(Gz3,axes=[0],keep_dims=True)\n",
    "        Gbatch2=tf.nn.batch_normalization(Gz3,Gmean2,Gvar2,Gbeta2,Ggamma2,1e-5,name='Gbatch2')\n",
    "        Ga3=tf.nn.relu(Gbatch2,name='Ga3')\n",
    "        #Deconv3\n",
    "        GW4=tf.get_variable(\"GW4\",[3,3,192,256],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gb4=tf.get_variable(\"Gb4\",[192],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        Goutshape3=[tf.shape(Ga3)[0],112,112,192]\n",
    "        Gdeconv3=tf.nn.conv2d_transpose(Ga3,GW4,output_shape=Goutshape3,strides=[1,4,4,1],padding=\"SAME\",name=\"Gdeconv3\")\n",
    "        Gz4=tf.nn.bias_add(Gdeconv3,Gb4,name='Gz4')\n",
    "        Ggamma3=tf.get_variable(\"Ggamma3\",[1],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gbeta3=tf.get_variable(\"Gbeta3\",[1],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gmean3,Gvar3=tf.nn.moments(Gz4,axes=[0],keep_dims=True)\n",
    "        Gbatch3=tf.nn.batch_normalization(Gz4,Gmean3,Gvar3,Gbeta3,Ggamma3,1e-5,name='Gbatch3')\n",
    "        Ga4=tf.nn.relu(Gbatch3,name='Ga4')\n",
    "        #Deconv4\n",
    "        GW5=tf.get_variable(\"GW5\",[3,3,3,192],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gb5=tf.get_variable(\"Gb5\",[3],initializer=tf.constant_initializer(0.0),dtype=tf.float32)\n",
    "        Goutshape4=[tf.shape(Ga4)[0],224,224,3]\n",
    "        Gdeconv4=tf.nn.conv2d_transpose(Ga4,GW5,output_shape=Goutshape4,strides=[1,2,2,1],padding=\"SAME\",name=\"Gdeconv4\")\n",
    "        Gz5=tf.nn.bias_add(Gdeconv4,Gb5,name='Gz5')\n",
    "        Ggamma4=tf.get_variable(\"Ggamma4\",[1],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gbeta4=tf.get_variable(\"Gbeta4\",[1],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        Gmean4,Gvar4=tf.nn.moments(Gz5,axes=[0],keep_dims=True)\n",
    "        Gbatch4=tf.nn.batch_normalization(Gz5,Gmean4,Gvar4,Gbeta4,Ggamma4,1e-5,name='Gbatch4')\n",
    "        Ga5=tf.nn.tanh(Gbatch4,name='Ga5')\n",
    "    return Ga5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(tf.report_uninitialized_variables()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out=tf.summary.FileWriter('./logs',sess.graph)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1=discriminator(sess=sess)\n",
    "D2=discriminator(reuse=True,sess=sess)\n",
    "D3=discriminator(Images=G,reuse=True,sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tf.placeholder(tf.float32,shape=(None,224,224,23),name='Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloss_1=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=D1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloss_2=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(D2[:,:,:,0]),logits=D2[:,:,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloss_3=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(D3[:,:,:,0]),logits=D3[:,:,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloss=dloss_1+dloss_2+dloss_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(D3[:,:,:,0]),logits=D3[:,:,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'add_1:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'Mean_3:0' shape=() dtype=float32>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dloss,gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabelledInput = tf.get_default_graph().get_tensor_by_name('Dinput:0')\n",
    "UnlabelledInput = tf.get_default_graph().get_tensor_by_name('Dinput_1:0')\n",
    "GeneratorInput = tf.get_default_graph().get_tensor_by_name('GInput:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabelledImages=[]\n",
    "for i in os.listdir(\"E:\\\\DIP\\\\VOCtrain\\\\VOCdevkit\\\\VOC2012\\\\SegmentationClass\\\\\"):\n",
    "    LabelledImages.append(i[:-3])\n",
    "UnlabelledImages=[]\n",
    "for i in os.listdir(\"E:\\\\DIP\\\\VOCtrain\\\\VOCdevkit\\\\VOC2012\\\\JPEGImages\\\\\"):\n",
    "    UnlabelledImages.append(i[:-3])\n",
    "UnlabelledImages=list(set(UnlabelledImages)-set(LabelledImages))\n",
    "UnlabelledImages=[i for i in UnlabelledImages]\n",
    "LabelledImages=[i for i in LabelledImages] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dlist=[i for i in tf.get_default_graph().get_collection_ref(tf.GraphKeys.TRAINABLE_VARIABLES) if i.name[0]=='D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Glist=[i for i in tf.get_default_graph().get_collection_ref(tf.GraphKeys.TRAINABLE_VARIABLES) if i.name[0]=='G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Variable 'DW14:0' shape=(3, 3, 128, 512) dtype=float32_ref>,\n",
       "  <tf.Variable 'Db14:0' shape=(128,) dtype=float32_ref>,\n",
       "  <tf.Variable 'DW15:0' shape=(3, 3, 32, 128) dtype=float32_ref>,\n",
       "  <tf.Variable 'Db15:0' shape=(32,) dtype=float32_ref>,\n",
       "  <tf.Variable 'DW16:0' shape=(3, 3, 23, 32) dtype=float32_ref>,\n",
       "  <tf.Variable 'Db16:0' shape=(23,) dtype=float32_ref>],\n",
       " [<tf.Variable 'GW1:0' shape=(100, 37681) dtype=float32_ref>,\n",
       "  <tf.Variable 'Gb1:0' shape=(37681,) dtype=float32_ref>,\n",
       "  <tf.Variable 'GW2:0' shape=(3, 3, 384, 769) dtype=float32_ref>,\n",
       "  <tf.Variable 'Gb2:0' shape=(384,) dtype=float32_ref>,\n",
       "  <tf.Variable 'Ggamma1:0' shape=(1,) dtype=float32_ref>,\n",
       "  <tf.Variable 'Gbeta1:0' shape=(1,) dtype=float32_ref>,\n",
       "  <tf.Variable 'GW3:0' shape=(3, 3, 256, 384) dtype=float32_ref>,\n",
       "  <tf.Variable 'Gb3:0' shape=(256,) dtype=float32_ref>,\n",
       "  <tf.Variable 'Ggamma2:0' shape=(1,) dtype=float32_ref>,\n",
       "  <tf.Variable 'Gbeta2:0' shape=(1,) dtype=float32_ref>,\n",
       "  <tf.Variable 'GW4:0' shape=(3, 3, 192, 256) dtype=float32_ref>,\n",
       "  <tf.Variable 'Gb4:0' shape=(192,) dtype=float32_ref>,\n",
       "  <tf.Variable 'Ggamma3:0' shape=(1,) dtype=float32_ref>,\n",
       "  <tf.Variable 'Gbeta3:0' shape=(1,) dtype=float32_ref>,\n",
       "  <tf.Variable 'GW5:0' shape=(3, 3, 3, 192) dtype=float32_ref>,\n",
       "  <tf.Variable 'Gb5:0' shape=(3,) dtype=float32_ref>,\n",
       "  <tf.Variable 'Ggamma4:0' shape=(1,) dtype=float32_ref>,\n",
       "  <tf.Variable 'Gbeta4:0' shape=(1,) dtype=float32_ref>])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dlist,Glist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "weights=np.load(\"E:\\\\DIP\\\\vgg16_weights.npz\")\n",
    "j=0\n",
    "Parameters=[i for i in tf.get_default_graph().get_collection_ref(tf.GraphKeys.GLOBAL_VARIABLES) if i.name[0]=='D']\n",
    "for i in sorted(weights.keys()):\n",
    "    if j>=26:\n",
    "        break\n",
    "    else:\n",
    "        sess.run(Parameters[j].assign(weights[i]))\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DiscTrain=tf.train.AdamOptimizer(0.00001).minimize(dloss,var_list=Dlist)\n",
    "GenTrain=tf.train.AdamOptimizer(0.000000002).minimize(gloss,var_list=Glist)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofepochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "0 0 3.8737712 0.6802483\n",
      "0 2 4.1280766 0.6802459\n",
      "0 4 4.052579 0.68024415\n",
      "0 6 3.9981136 0.68024224\n",
      "0 8 4.249869 0.6802432\n",
      "0 10 4.291905 0.6802453\n",
      "0 12 3.9297554 0.6802453\n",
      "0 14 4.065435 0.6802432\n",
      "0 16 4.056595 0.6802406\n",
      "0 18 4.0479593 0.68023837\n",
      "0 20 4.078666 0.68023634\n",
      "0 22 4.034344 0.68023455\n",
      "0 24 3.8949175 0.6802316\n",
      "0 26 4.511838 0.68023354\n",
      "0 28 3.6289518 0.6802312\n",
      "0 30 3.7571034 0.680227\n",
      "0 32 4.2608294 0.6802256\n",
      "0 34 3.825182 0.6802231\n",
      "0 36 3.9123306 0.6802214\n",
      "0 38 4.1200986 0.6802198\n",
      "0 40 4.0272274 0.68021804\n",
      "0 42 4.0345573 0.68021715\n",
      "0 44 3.6516707 0.68021286\n",
      "0 46 4.1299405 0.6802091\n",
      "0 48 4.029875 0.68020624\n",
      "0 50 3.9866972 0.6802035\n",
      "0 52 3.844334 0.6802\n",
      "0 54 3.9478524 0.6801961\n",
      "0 56 3.8179464 0.68018997\n",
      "0 58 3.9212914 0.68018144\n",
      "0 60 4.0712223 0.6801748\n",
      "0 62 3.9485486 0.6801687\n",
      "0 64 3.697466 0.6801611\n",
      "0 66 3.929854 0.6801525\n",
      "0 68 3.9088988 0.6801427\n",
      "0 70 4.035878 0.6801339\n",
      "0 72 4.000942 0.6801242\n",
      "0 74 3.859721 0.6801157\n",
      "0 76 4.042074 0.68010855\n",
      "0 78 3.8889086 0.6801001\n",
      "0 80 3.938824 0.6800911\n",
      "0 82 4.0645504 0.68008405\n",
      "0 84 3.9427934 0.6800766\n",
      "0 86 3.8876 0.6800682\n",
      "0 88 3.9837863 0.68005913\n",
      "0 90 4.2540283 0.68005276\n",
      "0 92 3.996431 0.68004787\n",
      "0 94 3.6796377 0.6800397\n",
      "0 96 3.8512712 0.68003035\n",
      "0 98 4.3794794 0.68002504\n",
      "0 100 3.7798157 0.68001837\n",
      "0 102 4.3350163 0.6800137\n",
      "0 104 4.1034694 0.6800103\n",
      "0 106 3.854257 0.6800057\n",
      "0 108 4.40147 0.6800041\n",
      "0 110 3.979849 0.6800003\n",
      "0 112 4.214535 0.6799986\n",
      "0 114 4.4357195 0.6800008\n",
      "0 116 3.8453195 0.68000126\n",
      "0 118 3.5198307 0.6799983\n",
      "0 120 3.9903636 0.6799954\n",
      "0 122 3.9134917 0.6799927\n",
      "0 124 4.2507277 0.6799917\n",
      "0 126 4.0562687 0.6799911\n",
      "0 128 3.709537 0.679987\n",
      "0 130 3.9477868 0.67998195\n",
      "0 132 3.839903 0.67997503\n",
      "0 134 4.0584946 0.67996883\n",
      "0 136 3.9827902 0.6799637\n",
      "0 138 4.1102138 0.67995995\n",
      "0 140 3.775021 0.6799551\n",
      "0 142 4.101164 0.67995125\n",
      "0 144 3.9356723 0.6799466\n",
      "0 146 3.6655996 0.67993927\n",
      "0 148 4.0511084 0.6799335\n",
      "0 150 4.0256195 0.67992914\n",
      "0 152 3.54034 0.6799212\n",
      "0 154 3.9187312 0.6799125\n",
      "0 156 3.93037 0.6799049\n",
      "0 158 4.2404847 0.6798999\n",
      "0 160 4.2565174 0.6798969\n",
      "0 162 4.3001914 0.67989707\n",
      "0 164 4.38711 0.67990005\n",
      "0 166 4.121891 0.6799041\n",
      "0 168 3.699108 0.679905\n",
      "0 170 3.9548364 0.6799047\n",
      "0 172 3.9762988 0.67990464\n",
      "0 174 4.019432 0.6799049\n",
      "0 176 3.546732 0.6799024\n",
      "0 178 3.8918684 0.6798978\n",
      "0 180 3.8273225 0.6798927\n",
      "0 182 4.2286363 0.6798911\n",
      "0 184 4.190902 0.6798912\n",
      "0 186 3.912185 0.6798899\n",
      "0 188 3.8820512 0.67988557\n",
      "0 190 3.8371577 0.6798812\n",
      "0 192 3.9359307 0.67987573\n",
      "0 194 3.989078 0.67987055\n",
      "0 196 3.8335967 0.6798642\n",
      "0 198 3.6420012 0.6798561\n",
      "0 200 4.11003 0.67984974\n",
      "0 202 4.041063 0.6798445\n",
      "0 204 3.904366 0.6798396\n",
      "0 206 4.1332135 0.6798362\n",
      "0 208 3.8906307 0.67983204\n",
      "0 210 4.1860104 0.6798306\n",
      "0 212 4.0972605 0.67983043\n",
      "0 214 4.2981977 0.67983264\n",
      "0 216 3.8235931 0.67983377\n",
      "0 218 3.6799343 0.67983294\n",
      "0 220 3.979913 0.67983073\n",
      "0 222 4.156618 0.67982966\n",
      "0 224 4.178858 0.6798309\n",
      "0 226 3.751179 0.6798295\n",
      "0 228 3.9026365 0.67982626\n",
      "0 230 3.8461263 0.6798213\n",
      "0 232 4.0185723 0.6798175\n",
      "0 234 3.8398755 0.6798134\n",
      "0 236 3.923976 0.67980856\n",
      "0 238 3.7730725 0.6798027\n",
      "0 240 3.8993475 0.67979616\n",
      "0 242 4.196496 0.67979205\n",
      "0 244 4.1284213 0.6797884\n",
      "0 246 4.2688904 0.679787\n",
      "0 248 4.0230527 0.6797857\n",
      "0 250 3.7402697 0.67978\n",
      "0 252 3.7975636 0.67977345\n",
      "0 254 4.152189 0.67976767\n",
      "0 256 4.1605263 0.6797641\n",
      "0 258 3.9618838 0.6797598\n",
      "0 260 4.019628 0.6797557\n",
      "0 262 3.8773246 0.67975104\n",
      "0 264 3.8551216 0.6797463\n",
      "0 266 3.7248302 0.6797411\n",
      "0 268 3.8443477 0.67973375\n",
      "0 270 4.5068297 0.6797311\n",
      "0 272 3.8397677 0.6797279\n",
      "0 274 3.8279657 0.679723\n",
      "0 276 4.087836 0.67971957\n",
      "0 278 4.266001 0.6797192\n",
      "0 280 4.12187 0.67972076\n",
      "0 282 4.0591764 0.67972153\n",
      "0 284 3.9925663 0.67972153\n",
      "0 286 4.0929394 0.6797228\n",
      "0 288 3.7071254 0.67972213\n",
      "0 290 3.534209 0.6797182\n",
      "0 292 3.8258054 0.6797135\n",
      "0 294 3.4338093 0.6797053\n",
      "0 296 3.7472558 0.6796959\n",
      "0 298 4.046615 0.6796886\n",
      "0 300 3.7254176 0.6796808\n",
      "0 302 3.9658742 0.67967284\n",
      "0 304 3.9810462 0.6796654\n",
      "0 306 3.9371612 0.67965806\n",
      "0 308 3.9987674 0.6796503\n",
      "0 310 3.9045956 0.6796404\n",
      "0 312 3.5884109 0.67962795\n",
      "0 314 3.9587648 0.67961687\n",
      "0 316 3.5921063 0.6796036\n",
      "0 318 4.094375 0.67959255\n",
      "0 320 4.115464 0.6795835\n",
      "0 322 3.9194372 0.679575\n",
      "0 324 3.6077194 0.6795645\n",
      "0 326 3.6200428 0.67955214\n",
      "0 328 3.920973 0.6795407\n",
      "0 330 3.976008 0.67953104\n",
      "0 332 3.881278 0.67952275\n",
      "0 334 4.289921 0.6795173\n",
      "0 336 3.9194887 0.6795122\n",
      "0 338 3.6361701 0.6795066\n",
      "0 340 4.198961 0.67950284\n",
      "0 342 3.9129486 0.67950004\n",
      "0 344 4.1467266 0.67949885\n",
      "0 346 3.9134147 0.67949694\n",
      "0 348 4.1371374 0.6794963\n",
      "0 350 4.167189 0.6794976\n",
      "0 352 4.3552547 0.6795011\n",
      "0 354 4.0110307 0.67950255\n",
      "0 356 3.7101488 0.6795008\n",
      "0 358 3.8871257 0.6794982\n",
      "0 360 3.8848033 0.6794955\n",
      "0 362 4.0674424 0.67949456\n",
      "0 364 3.855548 0.67949224\n",
      "0 366 3.9459355 0.6794884\n",
      "0 368 3.9791706 0.67948514\n",
      "0 370 3.7745044 0.67948025\n",
      "0 372 3.7826128 0.6794734\n",
      "0 374 3.731477 0.67946595\n",
      "0 376 3.6438203 0.6794561\n",
      "0 378 4.0697756 0.6794481\n",
      "0 380 4.0745873 0.6794415\n",
      "0 382 3.7676122 0.67943364\n",
      "0 384 4.117477 0.6794275\n",
      "0 386 3.8988628 0.67942226\n",
      "0 388 4.1325006 0.67941916\n",
      "0 390 4.22828 0.67941916\n",
      "0 392 3.722149 0.67941713\n",
      "0 394 4.0167007 0.6794137\n",
      "0 396 3.835865 0.6794086\n",
      "0 398 3.8361006 0.679403\n",
      "0 400 3.842476 0.67939705\n",
      "0 402 3.7833276 0.6793907\n",
      "0 404 3.852609 0.6793828\n",
      "0 406 3.8607793 0.67937267\n",
      "0 408 3.9161954 0.6793632\n",
      "0 410 3.758202 0.6793526\n",
      "0 412 3.8407922 0.67934155\n",
      "0 414 4.3947144 0.6793356\n",
      "0 416 3.6212347 0.67932796\n",
      "0 418 4.1587467 0.6793209\n",
      "0 420 4.207614 0.679316\n",
      "0 422 4.0974083 0.67931306\n",
      "0 424 3.9801586 0.67931086\n",
      "0 426 4.445139 0.6793131\n",
      "0 428 3.7864654 0.6793126\n",
      "0 430 4.0997753 0.67931384\n",
      "0 432 3.8217301 0.6793124\n",
      "0 434 4.1324844 0.6793127\n",
      "0 436 3.915137 0.67931235\n",
      "0 438 4.089234 0.6793132\n",
      "0 440 4.174357 0.6793152\n",
      "0 442 4.155695 0.67931825\n",
      "0 444 3.8138375 0.6793189\n",
      "0 446 3.8676548 0.67931896\n",
      "0 448 3.9485984 0.6793205\n",
      "0 450 3.8258753 0.67932105\n",
      "0 452 4.02746 0.67932206\n",
      "0 454 4.072022 0.6793248\n",
      "0 456 4.2756424 0.6793299\n",
      "0 458 3.8010185 0.6793325\n",
      "0 460 4.305055 0.67933875\n",
      "0 462 3.607199 0.6793413\n",
      "0 464 3.6427445 0.6793414\n",
      "0 466 4.3252006 0.6793434\n",
      "0 468 4.163344 0.6793465\n",
      "0 470 3.745557 0.679347\n",
      "0 472 3.9329758 0.6793484\n",
      "0 474 4.1063733 0.6793502\n",
      "0 476 3.8895373 0.6793514\n",
      "0 478 4.1590247 0.67935324\n",
      "0 480 4.045092 0.6793556\n",
      "0 482 4.091124 0.6793597\n",
      "0 484 3.6059031 0.6793613\n",
      "0 486 4.177626 0.67936486\n",
      "0 488 4.4742866 0.6793713\n",
      "0 490 3.6101627 0.6793742\n",
      "0 492 3.7437706 0.6793753\n",
      "0 494 4.0060635 0.6793775\n",
      "0 496 3.830687 0.6793781\n",
      "0 498 3.8831406 0.6793782\n",
      "0 500 3.8795002 0.6793768\n",
      "0 502 4.01853 0.67937547\n",
      "0 504 4.0758414 0.67937475\n",
      "0 506 3.6233048 0.67937064\n",
      "0 508 3.9695606 0.67936677\n",
      "0 510 3.6233084 0.6793606\n",
      "0 512 3.8771343 0.6793544\n",
      "0 514 3.6053603 0.6793462\n",
      "0 516 4.0367136 0.6793395\n",
      "0 518 4.149674 0.6793351\n",
      "0 520 3.6774278 0.6793291\n",
      "0 522 3.8778195 0.6793235\n",
      "0 524 3.7645488 0.6793177\n",
      "0 526 3.792036 0.6793107\n",
      "0 528 3.9665215 0.679305\n",
      "0 530 4.1905828 0.6793016\n",
      "0 532 3.670389 0.6792956\n",
      "0 534 3.8545995 0.6792889\n",
      "0 536 3.7820663 0.6792814\n",
      "0 538 3.8427885 0.6792732\n",
      "0 540 3.4390385 0.6792631\n",
      "0 542 3.617993 0.6792514\n",
      "0 544 3.849679 0.67924136\n",
      "0 546 3.692951 0.6792305\n",
      "0 548 4.1480913 0.67922115\n",
      "0 550 4.047925 0.67921275\n",
      "0 552 4.1319904 0.6792054\n",
      "0 554 3.6141393 0.67919767\n",
      "0 556 4.02335 0.67919135\n",
      "0 558 3.79957 0.6791845\n",
      "0 560 4.2816963 0.6791807\n",
      "0 562 4.2212815 0.6791787\n",
      "0 564 4.2012234 0.67917895\n",
      "0 566 4.072735 0.6791804\n",
      "0 568 3.8811858 0.67918193\n",
      "0 570 4.1539583 0.6791863\n",
      "0 572 3.649118 0.67918724\n",
      "0 574 3.519406 0.6791855\n",
      "0 576 3.6361742 0.67918235\n",
      "0 578 3.4612088 0.6791765\n",
      "0 580 3.7294788 0.6791692\n",
      "0 582 4.037426 0.6791631\n",
      "0 584 3.915939 0.67915857\n",
      "0 586 3.7460856 0.67915297\n",
      "0 588 3.7733545 0.67914563\n",
      "0 590 3.5773983 0.679136\n",
      "0 592 3.877144 0.6791277\n",
      "0 594 3.7887664 0.6791187\n",
      "0 596 3.7470524 0.6791091\n",
      "0 598 4.069985 0.6791009\n",
      "0 600 3.55064 0.67908967\n",
      "0 602 4.0131836 0.67907834\n",
      "0 604 4.0293217 0.67906946\n",
      "0 606 3.8487964 0.6790608\n",
      "0 608 3.8473403 0.67905223\n",
      "0 610 4.6646743 0.6790513\n",
      "0 612 3.884603 0.6790499\n",
      "0 614 4.2310104 0.67905015\n",
      "0 616 4.010449 0.679051\n",
      "0 618 4.0856514 0.67905456\n",
      "0 620 3.7568212 0.6790566\n",
      "0 622 3.7219987 0.67905647\n",
      "0 624 4.1641107 0.6790588\n",
      "0 626 3.8252335 0.6790589\n",
      "0 628 3.7725167 0.6790579\n",
      "0 630 4.0840573 0.67905784\n",
      "0 632 3.6614127 0.6790552\n",
      "0 634 4.3001175 0.67905533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 636 3.7104447 0.679053\n",
      "0 638 3.7773948 0.6790495\n",
      "0 640 3.9454882 0.6790452\n",
      "0 642 3.8783927 0.6790416\n",
      "0 644 3.8161874 0.67903775\n",
      "0 646 4.007009 0.6790347\n",
      "0 648 3.7784925 0.67903006\n",
      "0 650 4.268382 0.6790291\n",
      "0 652 3.6148345 0.67902595\n",
      "0 654 4.1058407 0.6790225\n",
      "0 656 3.9155946 0.6790186\n",
      "0 658 3.9494805 0.6790143\n",
      "0 660 3.6501896 0.67900705\n",
      "0 662 3.7808645 0.6789988\n",
      "0 664 4.0286818 0.6789917\n",
      "0 666 3.6740787 0.6789833\n",
      "0 668 3.7424586 0.67897475\n",
      "0 670 3.681675 0.6789659\n",
      "0 672 3.9302177 0.6789585\n",
      "0 674 3.774003 0.67894995\n",
      "0 676 3.8966296 0.6789421\n",
      "0 678 3.8277235 0.6789347\n",
      "0 680 4.0332108 0.67892915\n",
      "0 682 3.8351927 0.67892283\n",
      "0 684 3.854228 0.6789182\n",
      "0 686 3.720263 0.67891294\n",
      "0 688 3.6784177 0.6789068\n",
      "0 690 3.8202069 0.6789007\n",
      "0 692 3.4866636 0.6788926\n",
      "0 694 4.072811 0.67888623\n",
      "0 696 3.4768095 0.67887557\n",
      "0 698 4.0422764 0.6788655\n",
      "0 700 3.8435497 0.6788547\n",
      "0 702 3.6808147 0.67884284\n",
      "0 704 4.170417 0.67883253\n",
      "0 706 3.8170033 0.6788221\n",
      "0 708 3.9885843 0.67881244\n",
      "0 710 3.753615 0.6788035\n",
      "0 712 3.5838099 0.6787916\n",
      "0 714 4.0224686 0.6787802\n",
      "0 716 3.8256767 0.67876935\n",
      "0 718 3.7507708 0.6787575\n",
      "0 720 3.999372 0.67874765\n",
      "0 722 3.8965487 0.6787389\n",
      "0 724 3.6067097 0.67872846\n",
      "0 726 3.7019172 0.6787175\n",
      "0 728 4.4187937 0.67871124\n",
      "0 730 3.729593 0.6787029\n",
      "0 732 3.909778 0.6786955\n",
      "0 734 3.9185889 0.6786882\n",
      "0 736 4.0758734 0.67868346\n",
      "0 738 3.718449 0.67867666\n",
      "0 740 3.5960658 0.67866874\n",
      "0 742 3.7803404 0.67866033\n",
      "0 744 3.7219262 0.67865074\n",
      "0 746 3.6672983 0.67864007\n",
      "0 748 4.08695 0.678632\n",
      "0 750 3.8785157 0.67862403\n",
      "0 752 3.92866 0.6786163\n",
      "0 754 3.719205 0.6786066\n",
      "0 756 3.7429452 0.6785968\n",
      "0 758 3.9160168 0.6785878\n",
      "0 760 3.6898773 0.67857903\n",
      "0 762 3.9631362 0.67857033\n",
      "0 764 3.580374 0.6785604\n",
      "0 766 4.0971823 0.67855304\n",
      "0 768 3.738009 0.6785437\n",
      "0 770 3.388256 0.6785308\n",
      "0 772 3.5494528 0.6785172\n",
      "0 774 3.9485824 0.6785042\n",
      "0 776 3.819377 0.6784923\n",
      "0 778 3.9989836 0.6784825\n",
      "0 780 3.7462654 0.67847186\n",
      "0 782 3.6430035 0.67846155\n",
      "0 784 3.911214 0.6784508\n",
      "0 786 3.8282108 0.67843986\n",
      "0 788 3.6859245 0.67842746\n",
      "0 790 3.8866549 0.67841667\n",
      "0 792 3.918462 0.6784078\n",
      "0 794 3.7246418 0.67839754\n",
      "0 796 3.983753 0.67838895\n",
      "0 798 3.8209887 0.6783796\n",
      "0 800 3.8786685 0.6783701\n",
      "0 802 3.6425087 0.67835957\n",
      "0 804 3.9414325 0.67834955\n",
      "0 806 3.7639909 0.6783356\n",
      "0 808 3.5675154 0.67832154\n",
      "0 810 4.0928583 0.6783094\n",
      "0 812 3.8997777 0.6782972\n",
      "0 814 3.7396638 0.67828524\n",
      "0 816 3.7555337 0.6782731\n",
      "0 818 4.274162 0.6782656\n",
      "0 820 3.8280838 0.6782583\n",
      "0 822 3.8977408 0.67825156\n",
      "0 824 3.7428823 0.6782419\n",
      "0 826 3.8296943 0.6782303\n",
      "0 828 3.842481 0.6782183\n",
      "0 830 4.0683928 0.67820936\n",
      "0 832 3.6636386 0.6781986\n",
      "0 834 3.787916 0.67818934\n",
      "0 836 3.867068 0.6781809\n",
      "0 838 4.128008 0.6781747\n",
      "0 840 4.0415525 0.6781707\n",
      "0 842 3.9003894 0.6781665\n",
      "0 844 3.8799238 0.67816055\n",
      "0 846 4.0439873 0.6781563\n",
      "0 848 3.670635 0.67814916\n",
      "0 850 3.9211228 0.67814326\n",
      "0 852 3.925723 0.6781367\n",
      "0 854 4.0211797 0.6781313\n",
      "0 856 4.349288 0.67813087\n",
      "0 858 3.7609193 0.6781296\n",
      "0 860 3.8575187 0.67813015\n",
      "0 862 3.602725 0.6781276\n",
      "0 864 3.8857315 0.6781239\n",
      "0 866 3.843428 0.6781207\n",
      "0 868 3.7518795 0.6781176\n",
      "0 870 4.1135592 0.6781158\n",
      "0 872 3.8184216 0.6781135\n",
      "0 874 3.849317 0.6781111\n",
      "0 876 4.4411063 0.6781132\n",
      "0 878 3.927034 0.67811584\n",
      "0 880 3.5556903 0.6781141\n",
      "0 882 3.890928 0.67811084\n",
      "0 884 3.8783994 0.67810696\n",
      "0 886 3.501925 0.6780984\n",
      "0 888 3.6831439 0.6780906\n",
      "0 890 4.0378227 0.6780861\n",
      "0 892 4.32054 0.6780858\n",
      "0 894 3.9065204 0.67808455\n",
      "0 896 4.0936284 0.6780858\n",
      "0 898 3.8513799 0.6780866\n",
      "0 900 3.9146662 0.67808753\n",
      "0 902 4.190889 0.6780907\n",
      "0 904 3.926643 0.678093\n",
      "0 906 3.9628985 0.67809623\n",
      "0 908 3.8963964 0.6780991\n",
      "0 910 3.61866 0.6781003\n",
      "0 912 4.123162 0.6781031\n",
      "0 914 3.679913 0.6781034\n",
      "0 916 3.9531956 0.67810416\n",
      "0 918 3.5112753 0.678101\n",
      "0 920 3.909172 0.67809796\n",
      "0 922 4.0344753 0.67809683\n",
      "0 924 3.814112 0.6780956\n",
      "0 926 3.7729821 0.67809415\n",
      "0 928 3.9383972 0.67809176\n",
      "0 930 4.026804 0.6780914\n",
      "0 932 3.8885217 0.67809033\n",
      "0 934 3.7301393 0.6780879\n",
      "0 936 3.6378036 0.6780833\n",
      "0 938 3.806369 0.6780784\n",
      "0 940 4.1261234 0.67807496\n",
      "0 942 4.196069 0.67807627\n",
      "0 944 3.7879968 0.67807686\n",
      "0 946 3.6642704 0.6780751\n",
      "0 948 4.0612946 0.67807394\n",
      "0 950 3.800532 0.6780723\n",
      "0 952 3.8648944 0.678071\n",
      "0 954 3.8889081 0.67807007\n",
      "0 956 3.704431 0.67806774\n",
      "0 958 4.052793 0.6780671\n",
      "0 960 4.2257223 0.6780699\n",
      "0 962 4.004829 0.6780724\n",
      "0 964 3.8780384 0.67807364\n",
      "0 966 4.514901 0.6780799\n",
      "0 968 3.986206 0.67808586\n",
      "0 970 4.1995764 0.6780939\n",
      "0 972 3.725731 0.6781003\n",
      "0 974 3.811167 0.67810494\n",
      "0 976 3.6521087 0.678106\n",
      "0 978 3.9700181 0.6781077\n",
      "0 980 3.589179 0.6781069\n",
      "0 982 3.9514937 0.67810714\n",
      "0 984 4.2917767 0.6781104\n",
      "0 986 3.9437146 0.6781121\n",
      "0 988 4.318154 0.6781173\n",
      "0 990 4.150073 0.6781244\n",
      "0 992 3.4366672 0.67812675\n",
      "0 994 4.229199 0.67813027\n",
      "0 996 4.2352114 0.6781364\n",
      "0 998 3.895827 0.67814285\n",
      "0 1000 4.3456583 0.67815214\n",
      "0 1002 3.6567202 0.6781594\n",
      "0 1004 4.0993257 0.6781685\n",
      "0 1006 3.4653056 0.67817277\n",
      "0 1008 4.494537 0.6781813\n",
      "0 1010 3.9334078 0.67818874\n",
      "0 1012 4.07136 0.67819613\n",
      "0 1014 3.7357502 0.67820305\n",
      "0 1016 4.2567773 0.67821145\n",
      "0 1018 3.804441 0.6782186\n",
      "0 1020 4.246682 0.6782281\n",
      "0 1022 4.2424493 0.6782401\n",
      "0 1024 3.7762043 0.67825085\n",
      "0 1026 3.3563232 0.6782574\n",
      "0 1028 3.6958375 0.6782618\n",
      "0 1030 3.6250434 0.6782647\n",
      "0 1032 4.207167 0.6782714\n",
      "0 1034 4.2388253 0.6782802\n",
      "0 1036 3.7262478 0.67828697\n",
      "0 1038 4.3040524 0.6782971\n",
      "0 1040 4.0891 0.6783093\n",
      "0 1042 4.1523895 0.6783223\n",
      "0 1044 3.6718264 0.67833334\n",
      "0 1046 3.6888595 0.6783422\n",
      "0 1048 4.28771 0.6783528\n",
      "0 1050 4.368481 0.67836654\n",
      "0 1052 3.9804282 0.6783806\n",
      "0 1054 3.76069 0.6783911\n",
      "0 1056 3.69093 0.67840075\n",
      "0 1058 3.4591584 0.6784056\n",
      "0 1060 3.9745188 0.67841107\n",
      "0 1062 3.7155025 0.6784151\n",
      "0 1064 4.437151 0.67842484\n",
      "0 1066 4.3264666 0.678435\n",
      "0 1068 3.4962013 0.67844194\n",
      "0 1070 3.7572563 0.67844754\n",
      "0 1072 4.157482 0.67845595\n",
      "0 1074 3.9270804 0.678465\n",
      "0 1076 3.5869622 0.6784706\n",
      "0 1078 4.15474 0.678478\n",
      "0 1080 4.1175046 0.67848533\n",
      "0 1082 3.5632806 0.6784903\n",
      "0 1084 3.832837 0.67849433\n",
      "0 1086 4.2399035 0.67850107\n",
      "0 1088 4.1805654 0.67850953\n",
      "0 1090 3.8170226 0.6785169\n",
      "0 1092 3.7700558 0.6785235\n",
      "0 1094 3.8959656 0.67852765\n",
      "0 1096 3.999714 0.67853236\n",
      "0 1098 3.689526 0.6785342\n",
      "0 1100 3.6517959 0.67853343\n",
      "0 1102 4.425361 0.6785375\n",
      "0 1104 3.6836255 0.6785399\n",
      "0 1106 4.2547827 0.6785444\n",
      "0 1108 3.8191426 0.678548\n",
      "0 1110 3.864046 0.67854965\n",
      "0 1112 4.15472 0.67855245\n",
      "0 1114 4.121591 0.67855614\n",
      "0 1116 3.5520077 0.6785548\n",
      "0 1118 4.454773 0.6785591\n",
      "0 1120 4.2265434 0.6785652\n",
      "0 1122 3.968522 0.6785705\n",
      "0 1124 4.584125 0.67858094\n",
      "0 1126 4.002009 0.6785912\n",
      "0 1128 3.7125633 0.6785996\n",
      "0 1130 4.398427 0.6786115\n",
      "0 1132 4.066259 0.6786237\n",
      "0 1134 4.082181 0.6786356\n",
      "0 1136 3.537281 0.678642\n",
      "0 1138 3.9054165 0.678645\n",
      "0 1140 4.1124907 0.6786497\n",
      "0 1142 3.7041821 0.6786529\n",
      "0 1144 4.1769853 0.67865723\n",
      "0 1146 4.073746 0.67866194\n",
      "0 1148 4.230656 0.67866975\n",
      "0 1150 4.175389 0.6786797\n",
      "0 1152 3.839864 0.67868847\n",
      "0 1154 4.3140364 0.67869973\n",
      "0 1156 3.9215174 0.6787101\n",
      "0 1158 3.8708055 0.6787178\n",
      "0 1160 3.4736362 0.67872083\n",
      "0 1162 3.8583746 0.6787237\n",
      "0 1164 3.7971408 0.6787257\n",
      "0 1166 4.113349 0.6787294\n",
      "0 1168 3.957573 0.6787326\n",
      "0 1170 3.740113 0.6787343\n",
      "0 1172 3.934005 0.6787356\n",
      "0 1174 3.7690403 0.6787343\n",
      "0 1176 4.4210205 0.6787389\n",
      "0 1178 3.7098322 0.6787425\n",
      "0 1180 3.4976764 0.67874223\n",
      "0 1182 3.9040635 0.67874175\n",
      "0 1184 4.000111 0.6787432\n",
      "0 1186 4.29874 0.6787486\n",
      "0 1188 3.8898647 0.6787542\n",
      "0 1190 3.7199216 0.6787583\n",
      "0 1192 4.166006 0.67876416\n",
      "0 1194 3.5751007 0.67876714\n",
      "0 1196 4.263808 0.6787737\n",
      "0 1198 4.13407 0.6787807\n",
      "0 1200 3.67102 0.67878526\n",
      "0 1202 3.617982 0.6787874\n",
      "0 1204 3.8363776 0.6787875\n",
      "0 1206 4.07512 0.6787868\n",
      "0 1208 3.8402889 0.67878664\n",
      "0 1210 3.891512 0.6787837\n",
      "0 1212 3.9405746 0.67878115\n",
      "0 1214 3.5459268 0.6787778\n",
      "0 1216 3.7709632 0.67877156\n",
      "0 1218 3.708538 0.6787639\n",
      "0 1220 3.871953 0.67875665\n",
      "0 1222 4.325014 0.6787535\n",
      "0 1224 3.7286992 0.6787491\n",
      "0 1226 3.844684 0.6787452\n",
      "0 1228 4.4678826 0.6787461\n",
      "0 1230 3.6334965 0.6787427\n",
      "0 1232 4.2618604 0.67874277\n",
      "0 1234 3.8047965 0.67874175\n",
      "0 1236 4.2889724 0.67874503\n",
      "0 1238 3.7086227 0.6787452\n",
      "0 1240 4.1297164 0.6787476\n",
      "0 1242 4.0297093 0.6787476\n",
      "0 1244 3.5006378 0.6787432\n",
      "0 1246 4.029535 0.6787394\n",
      "0 1248 3.8227134 0.6787358\n",
      "0 1250 3.775497 0.6787315\n",
      "0 1252 4.097641 0.678729\n",
      "0 1254 4.4140167 0.67873144\n",
      "0 1256 3.756407 0.6787323\n",
      "0 1258 3.6687999 0.6787318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1260 4.045513 0.67873275\n",
      "0 1262 4.163373 0.6787359\n",
      "0 1264 4.061499 0.6787402\n",
      "0 1266 3.81344 0.6787434\n",
      "0 1268 4.133268 0.67874706\n",
      "0 1270 3.7389941 0.67874986\n",
      "0 1272 3.8331742 0.67875266\n",
      "0 1274 3.440683 0.67875206\n",
      "0 1276 3.8665423 0.67875135\n",
      "0 1278 3.6719556 0.67874926\n",
      "0 1280 3.9468148 0.67874676\n",
      "0 1282 4.177034 0.6787455\n",
      "0 1284 3.7327387 0.6787417\n",
      "0 1286 3.949574 0.6787395\n",
      "0 1288 4.0676227 0.67873925\n",
      "0 1290 3.5792685 0.6787366\n",
      "0 1292 3.6101882 0.67873055\n",
      "0 1294 3.785386 0.6787239\n",
      "0 1296 3.7541587 0.67871684\n",
      "0 1298 4.0115085 0.6787119\n",
      "0 1300 3.8556278 0.6787072\n",
      "0 1302 3.7977543 0.6787013\n",
      "0 1304 4.176685 0.6786977\n",
      "0 1306 3.8140228 0.67869425\n",
      "0 1308 3.9523172 0.6786919\n",
      "0 1310 3.9251442 0.67869055\n",
      "0 1312 4.194161 0.67869157\n",
      "0 1314 3.8439581 0.6786918\n",
      "0 1316 3.814199 0.67869085\n",
      "0 1318 3.7571175 0.6786898\n",
      "0 1320 3.6566253 0.6786868\n",
      "0 1322 3.8357172 0.67868173\n",
      "0 1324 3.4593856 0.6786762\n",
      "0 1326 3.8381658 0.67867106\n",
      "0 1328 3.579424 0.67866254\n",
      "0 1330 4.255098 0.67865807\n",
      "0 1332 3.9736726 0.67865556\n",
      "0 1334 3.690597 0.6786509\n",
      "0 1336 4.1188345 0.6786496\n",
      "0 1338 3.7729263 0.6786469\n",
      "0 1340 3.7626915 0.6786436\n",
      "0 1342 3.7031255 0.67864066\n",
      "0 1344 3.7900538 0.6786366\n",
      "0 1346 3.8193765 0.6786323\n",
      "0 1348 4.047994 0.6786292\n",
      "0 1350 3.6254215 0.67862517\n",
      "0 1352 3.806723 0.6786219\n",
      "0 1354 3.7879558 0.6786188\n",
      "0 1356 3.8709621 0.67861515\n",
      "0 1358 4.249905 0.67861503\n",
      "0 1360 4.06032 0.6786158\n",
      "0 1362 3.9235644 0.6786155\n",
      "0 1364 3.8477588 0.67861384\n",
      "0 1366 3.9428241 0.6786141\n",
      "0 1368 4.1542063 0.6786164\n",
      "0 1370 3.571082 0.6786165\n",
      "0 1372 4.227455 0.6786189\n",
      "0 1374 4.262015 0.67862517\n",
      "0 1376 3.21095 0.6786272\n",
      "0 1378 3.8264303 0.6786286\n",
      "0 1380 3.6820238 0.67862844\n",
      "0 1382 3.6328275 0.67862624\n",
      "0 1384 3.9189675 0.67862296\n",
      "0 1386 3.9511375 0.67861944\n",
      "0 1388 3.980238 0.6786169\n",
      "0 1390 3.8777235 0.678613\n",
      "0 1392 4.039781 0.6786108\n",
      "0 1394 4.081514 0.67860836\n",
      "0 1396 4.242104 0.67860895\n",
      "0 1398 3.7007363 0.6786074\n",
      "0 1400 4.09009 0.6786052\n",
      "0 1402 3.5472875 0.6786007\n",
      "0 1404 4.3588805 0.678601\n",
      "0 1406 4.138679 0.6786032\n",
      "0 1408 4.123305 0.67860764\n",
      "0 1410 4.1155143 0.67861307\n",
      "0 1412 4.657891 0.67862403\n",
      "0 1414 4.0946817 0.6786359\n",
      "0 1416 4.1795187 0.6786485\n",
      "0 1418 3.827117 0.6786577\n",
      "0 1420 3.6971593 0.6786646\n",
      "0 1422 3.8076892 0.67867094\n",
      "0 1424 3.5128453 0.67867297\n",
      "0 1426 3.7311478 0.6786724\n",
      "0 1428 3.9004664 0.678672\n",
      "0 1430 3.7544303 0.67867196\n",
      "0 1432 3.5046248 0.67867005\n",
      "0 1434 3.6205175 0.67866576\n",
      "0 1436 4.286648 0.678665\n",
      "0 1438 3.9250062 0.6786642\n",
      "0 1440 3.5676608 0.6786619\n",
      "0 1442 4.0450954 0.6786619\n",
      "0 1444 3.9174774 0.6786628\n",
      "0 1446 4.108138 0.67866695\n",
      "0 1448 4.057051 0.6786715\n",
      "0 1450 3.8659108 0.6786758\n",
      "0 1452 3.7339468 0.6786769\n",
      "0 1454 4.2078185 0.6786814\n",
      "0 1456 3.9700923 0.6786853\n",
      "0 1458 3.6854887 0.6786873\n",
      "0 1460 3.8950195 0.6786899\n",
      "0 1462 3.936233 0.67869234\n",
      "0 1464 3.6765285 0.6786911\n",
      "0 1466 4.041551 0.6786906\n",
      "0 1468 3.737886 0.6786899\n",
      "0 1470 3.956078 0.6786879\n",
      "0 1472 4.1203065 0.67868745\n",
      "0 1474 4.021052 0.67868835\n",
      "0 1476 3.9479547 0.6786906\n",
      "0 1478 4.1416955 0.67869425\n",
      "0 1480 3.9149776 0.67869896\n",
      "0 1482 3.9442937 0.67870224\n",
      "0 1484 3.8415914 0.67870456\n",
      "0 1486 4.26722 0.6787095\n",
      "0 1488 3.8916044 0.6787133\n",
      "0 1490 3.5141892 0.6787146\n",
      "0 1492 4.0693865 0.6787176\n",
      "0 1494 3.777433 0.6787201\n",
      "0 1496 4.356431 0.6787247\n",
      "0 1498 4.0242176 0.6787288\n",
      "0 1500 3.5863676 0.6787305\n",
      "0 1502 3.6621802 0.6787292\n",
      "0 1504 3.9097128 0.6787284\n",
      "0 1506 3.8802943 0.6787282\n",
      "0 1508 3.3916128 0.67872447\n",
      "0 1510 4.032834 0.67872214\n",
      "0 1512 4.2639556 0.6787234\n",
      "0 1514 4.087149 0.6787249\n",
      "0 1516 3.8199987 0.67872524\n",
      "0 1518 3.9184566 0.6787265\n",
      "0 1520 4.04405 0.6787297\n",
      "0 1522 4.2204466 0.6787371\n",
      "0 1524 4.326355 0.67874736\n",
      "0 1526 4.248762 0.67875934\n",
      "0 1528 3.843834 0.67876965\n",
      "0 1530 3.7335215 0.6787776\n",
      "0 1532 3.8457332 0.678785\n",
      "0 1534 3.5099025 0.6787888\n",
      "0 1536 3.8514974 0.67879266\n",
      "0 1538 4.0532036 0.6787982\n",
      "0 1540 3.4990711 0.6788\n",
      "0 1542 4.186474 0.6788047\n",
      "0 1544 3.9515083 0.6788081\n",
      "0 1546 3.866822 0.67881197\n",
      "0 1548 4.0803328 0.67881703\n",
      "0 1550 3.5633538 0.6788192\n",
      "0 1552 3.628212 0.6788182\n",
      "0 1554 3.8138132 0.67881525\n",
      "0 1556 4.347885 0.6788167\n",
      "0 1558 3.6170158 0.678816\n",
      "0 1560 3.9770794 0.67881525\n",
      "0 1562 3.9295373 0.6788152\n",
      "0 1564 3.627009 0.6788114\n",
      "0 1566 4.752279 0.67881614\n",
      "0 1568 3.981758 0.6788221\n",
      "0 1570 3.841447 0.6788272\n",
      "0 1572 3.8239415 0.67883044\n",
      "0 1574 3.7014248 0.6788292\n",
      "0 1576 3.7089787 0.6788257\n",
      "0 1578 4.041507 0.67882365\n",
      "0 1580 3.705904 0.67882085\n",
      "0 1582 3.9173775 0.6788192\n",
      "0 1584 4.1682053 0.6788198\n",
      "0 1586 4.017343 0.6788203\n",
      "0 1588 4.10255 0.67882395\n",
      "0 1590 3.757874 0.6788276\n",
      "0 1592 4.040752 0.67883253\n",
      "0 1594 4.055851 0.6788384\n",
      "0 1596 3.70406 0.67884123\n",
      "0 1598 4.3828645 0.678848\n",
      "0 1600 4.225395 0.67885685\n",
      "0 1602 3.6809363 0.67886275\n",
      "0 1604 3.298837 0.67886424\n",
      "0 1606 3.9157915 0.67886615\n",
      "0 1608 3.734736 0.6788668\n",
      "0 1610 3.7298777 0.67886573\n",
      "0 1612 3.995772 0.6788655\n",
      "0 1614 3.6403584 0.6788655\n",
      "0 1616 4.2765393 0.67886895\n",
      "0 1618 4.167592 0.678875\n",
      "0 1620 4.2123747 0.67888266\n",
      "0 1622 4.4139853 0.6788942\n",
      "0 1624 3.7658935 0.6789045\n",
      "0 1626 3.7875495 0.6789119\n",
      "0 1628 4.236725 0.67892236\n",
      "0 1630 4.0144076 0.6789315\n",
      "0 1632 3.8437524 0.67893875\n",
      "0 1634 3.9425044 0.6789462\n",
      "0 1636 3.7737808 0.6789521\n",
      "0 1638 4.0210633 0.6789583\n",
      "0 1640 3.9919639 0.6789658\n",
      "0 1642 4.0407224 0.6789746\n",
      "0 1644 3.7355132 0.67898047\n",
      "0 1646 3.8561265 0.6789848\n",
      "0 1648 3.964368 0.67898995\n",
      "0 1650 3.7384233 0.6789938\n",
      "0 1652 3.8884788 0.678998\n",
      "0 1654 4.032391 0.67900264\n",
      "0 1656 3.8104694 0.67900676\n",
      "0 1658 4.263838 0.6790138\n",
      "0 1660 4.0432014 0.6790212\n",
      "0 1662 4.031365 0.6790285\n",
      "0 1664 3.6421542 0.6790337\n",
      "0 1666 3.743311 0.679038\n",
      "0 1668 3.7521567 0.67904043\n",
      "0 1670 4.0651164 0.6790441\n",
      "0 1672 4.002899 0.67904866\n",
      "0 1674 3.6731431 0.6790513\n",
      "0 1676 4.152381 0.67905474\n",
      "0 1678 3.779156 0.6790565\n",
      "0 1680 3.8366094 0.67905754\n",
      "0 1682 3.8232656 0.67905736\n",
      "0 1684 3.7759018 0.67905766\n",
      "0 1686 3.7891612 0.67905784\n",
      "0 1688 4.360809 0.67906195\n",
      "0 1690 4.1581526 0.6790675\n",
      "0 1692 4.1136894 0.6790748\n",
      "0 1694 3.8001432 0.67908055\n",
      "0 1696 3.9970925 0.6790874\n",
      "0 1698 3.8443997 0.6790918\n",
      "0 1700 4.183384 0.6790967\n",
      "0 1702 4.1610527 0.6791039\n",
      "0 1704 4.4542913 0.6791146\n",
      "0 1706 3.836015 0.6791234\n",
      "0 1708 3.7740793 0.67912996\n",
      "0 1710 3.995637 0.67913723\n",
      "0 1712 3.77115 0.6791405\n",
      "0 1714 3.958832 0.67914474\n",
      "0 1716 4.013505 0.6791506\n",
      "0 1718 4.133204 0.6791581\n",
      "0 1720 3.934475 0.67916495\n",
      "0 1722 4.633869 0.6791771\n",
      "0 1724 4.421028 0.67919254\n",
      "0 1726 3.8487847 0.6792054\n",
      "0 1728 4.271077 0.6792207\n",
      "0 1730 3.922956 0.6792333\n",
      "0 1732 3.8703394 0.6792448\n",
      "0 1734 3.7890086 0.6792539\n",
      "0 1736 4.1928124 0.6792654\n",
      "0 1738 3.6805136 0.67927474\n",
      "0 1740 4.0332937 0.6792839\n",
      "0 1742 3.7229471 0.6792897\n",
      "0 1744 3.8993523 0.67929363\n",
      "0 1746 3.923266 0.67929584\n",
      "0 1748 4.0035753 0.6792984\n",
      "0 1750 3.6641989 0.6792989\n",
      "0 1752 3.8317163 0.67929864\n",
      "0 1754 3.8320675 0.6792979\n",
      "0 1756 3.7754207 0.6792959\n",
      "0 1758 4.0869865 0.6792962\n",
      "0 1760 3.767788 0.6792951\n",
      "0 1762 3.7927032 0.6792933\n",
      "0 1764 3.7961292 0.67929107\n",
      "0 1766 3.8365874 0.67929\n",
      "0 1768 4.2948694 0.67929226\n",
      "0 1770 3.745061 0.67929274\n",
      "0 1772 4.1078973 0.6792946\n",
      "0 1774 3.7851083 0.6792953\n",
      "0 1776 3.9863572 0.67929757\n",
      "0 1778 3.8666148 0.679299\n",
      "0 1780 4.1166368 0.67930216\n",
      "0 1782 4.0581365 0.67930627\n",
      "0 1784 3.6197493 0.67930907\n",
      "0 1786 4.2867594 0.6793157\n",
      "0 1788 4.195553 0.6793244\n",
      "0 1790 4.537955 0.679338\n",
      "0 1792 3.7493258 0.6793498\n",
      "0 1794 4.0885286 0.67936206\n",
      "0 1796 3.5551443 0.67937094\n",
      "0 1798 4.015943 0.67938\n",
      "0 1800 3.610909 0.6793857\n",
      "0 1802 4.040325 0.67939126\n",
      "0 1804 3.9212263 0.6793965\n",
      "0 1806 4.3449993 0.6794049\n",
      "0 1808 3.5778332 0.67940784\n",
      "0 1810 3.9384322 0.6794105\n",
      "0 1812 4.2943726 0.6794162\n",
      "0 1814 3.82877 0.6794205\n",
      "0 1816 4.286229 0.6794276\n",
      "0 1818 3.9120526 0.6794345\n",
      "0 1820 4.0208225 0.67944103\n",
      "0 1822 4.2189665 0.67944956\n",
      "0 1824 3.593482 0.67945474\n",
      "0 1826 4.0413957 0.67946064\n",
      "0 1828 3.936008 0.6794662\n",
      "0 1830 3.83981 0.6794709\n",
      "0 1832 3.67118 0.6794734\n",
      "0 1834 3.7390697 0.6794734\n",
      "0 1836 3.6594622 0.67947215\n",
      "0 1838 3.4679735 0.6794667\n",
      "0 1840 4.198461 0.67946297\n",
      "0 1842 3.9283276 0.6794602\n",
      "0 1844 3.6194098 0.67945474\n",
      "0 1846 4.0336986 0.6794506\n",
      "0 1848 4.192389 0.6794495\n",
      "0 1850 3.8311846 0.67944694\n",
      "0 1852 3.789767 0.6794436\n",
      "0 1854 4.2740145 0.67944354\n",
      "0 1856 3.8743148 0.67944336\n",
      "0 1858 3.8645935 0.6794436\n",
      "0 1860 3.7585661 0.67944103\n",
      "0 1862 4.177978 0.6794404\n",
      "0 1864 4.100147 0.67944103\n",
      "0 1866 3.76074 0.6794398\n",
      "0 1868 3.8899856 0.6794396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1870 3.8097415 0.6794391\n",
      "0 1872 4.0908375 0.679441\n",
      "0 1874 3.6844127 0.6794415\n",
      "0 1876 3.6360743 0.67944086\n",
      "0 1878 4.014613 0.67944103\n",
      "0 1880 3.4430997 0.6794378\n",
      "0 1882 4.0391817 0.6794367\n",
      "0 1884 3.6979804 0.6794344\n",
      "0 1886 4.1005244 0.6794341\n",
      "0 1888 4.335329 0.67943853\n",
      "0 1890 4.17613 0.6794451\n",
      "0 1892 4.105522 0.6794527\n",
      "0 1894 4.0491686 0.67946064\n",
      "0 1896 3.406477 0.67946345\n",
      "0 1898 3.9228299 0.67946607\n",
      "0 1900 3.9632006 0.67946863\n",
      "0 1902 3.878475 0.6794695\n",
      "0 1904 3.9006462 0.67947066\n",
      "0 1906 4.485142 0.67947716\n",
      "0 1908 4.022742 0.67948383\n",
      "0 1910 4.276152 0.6794929\n",
      "0 1912 3.659292 0.67949784\n",
      "0 1914 3.835742 0.6795019\n",
      "0 1916 4.1998878 0.6795097\n",
      "0 1918 3.850618 0.6795158\n",
      "0 1920 3.633334 0.67951715\n",
      "0 1922 4.071155 0.67951864\n",
      "0 1924 3.654589 0.67951727\n",
      "0 1926 4.145177 0.67951894\n",
      "0 1928 3.9533825 0.6795201\n",
      "0 1930 4.611045 0.679527\n",
      "0 1932 4.2001133 0.67953634\n",
      "0 1934 4.0447836 0.67954516\n",
      "0 1936 4.114852 0.67955554\n",
      "0 1938 3.5732374 0.679564\n",
      "0 1940 3.5866761 0.67957073\n",
      "0 1942 3.7966614 0.6795773\n",
      "0 1944 3.8856287 0.6795843\n",
      "0 1946 3.9701474 0.67958975\n",
      "0 1948 3.8938751 0.679594\n",
      "0 1950 3.4999127 0.679595\n",
      "0 1952 4.110398 0.67959815\n",
      "0 1954 4.066922 0.6796014\n",
      "0 1956 3.6346378 0.6796016\n",
      "0 1958 4.2109275 0.67960376\n",
      "0 1960 3.7366464 0.67960435\n",
      "0 1962 4.269957 0.67960715\n",
      "0 1964 3.8686507 0.6796088\n",
      "0 1966 3.8460476 0.6796104\n",
      "0 1968 3.7909658 0.6796112\n",
      "0 1970 3.8863556 0.6796107\n",
      "0 1972 3.8653529 0.6796108\n",
      "0 1974 3.9959233 0.679611\n",
      "0 1976 4.094525 0.67961323\n",
      "0 1978 3.7086146 0.6796145\n",
      "0 1980 3.2600489 0.6796115\n",
      "0 1982 4.120603 0.6796117\n",
      "0 1984 3.995997 0.6796112\n",
      "0 1986 4.125924 0.6796125\n",
      "0 1988 3.8649416 0.67961293\n",
      "0 1990 3.7003145 0.6796109\n",
      "0 1992 4.0382524 0.67961013\n",
      "0 1994 3.9273033 0.67960894\n",
      "0 1996 3.7488472 0.67960584\n",
      "0 1998 3.6001253 0.6796023\n",
      "0 2000 4.075973 0.6796007\n",
      "0 2002 3.7946851 0.67959934\n",
      "0 2004 3.8766687 0.67959845\n",
      "0 2006 3.7032082 0.6795969\n",
      "0 2008 4.101078 0.67959744\n",
      "0 2010 4.0190268 0.6795974\n",
      "0 2012 3.8202338 0.6795962\n",
      "0 2014 3.4717205 0.67959267\n",
      "0 2016 4.2735605 0.6795936\n",
      "0 2018 3.868329 0.67959285\n",
      "0 2020 3.8744957 0.6795917\n",
      "0 2022 3.8887875 0.67959034\n",
      "0 2024 4.1791625 0.67959315\n",
      "0 2026 3.7753186 0.6795947\n",
      "0 2028 4.0041127 0.679597\n",
      "0 2030 3.6279466 0.6795988\n",
      "0 2032 4.2244773 0.679603\n",
      "0 2034 3.868001 0.6796059\n",
      "0 2036 3.960192 0.67960936\n",
      "0 2038 3.6478608 0.6796109\n",
      "0 2040 4.093818 0.6796147\n",
      "0 2042 4.3924255 0.6796227\n",
      "0 2044 3.9617715 0.67963135\n",
      "0 2046 4.20266 0.6796422\n",
      "0 2048 3.9309251 0.679652\n",
      "0 2050 4.1631446 0.6796628\n",
      "0 2052 4.0937486 0.6796752\n",
      "0 2054 4.15397 0.6796886\n",
      "0 2056 3.5692844 0.6796957\n",
      "0 2058 3.966317 0.67970306\n",
      "0 2060 4.2554383 0.6797138\n",
      "0 2062 4.053754 0.6797246\n",
      "0 2064 3.7863069 0.6797333\n",
      "0 2066 3.7109482 0.6797406\n",
      "0 2068 4.165284 0.6797495\n",
      "0 2070 3.8877554 0.67975724\n",
      "0 2072 4.039423 0.67976636\n",
      "0 2074 3.5713384 0.6797719\n",
      "0 2076 3.579134 0.6797748\n",
      "0 2078 3.96231 0.67977965\n",
      "0 2080 3.482067 0.6797813\n",
      "0 2082 3.7154012 0.6797795\n",
      "0 2084 4.171768 0.6797803\n",
      "0 2086 4.359356 0.67978436\n",
      "0 2088 3.8631155 0.67978853\n",
      "0 2090 4.3799386 0.67979604\n",
      "0 2092 3.8618045 0.67980176\n",
      "0 2094 3.8193002 0.6798062\n",
      "0 2096 4.259912 0.6798138\n",
      "0 2098 3.7741716 0.6798202\n",
      "0 2100 3.8463373 0.67982745\n",
      "0 2102 3.7169228 0.67983323\n",
      "0 2104 3.9092984 0.67983824\n",
      "0 2106 4.3372254 0.67984676\n",
      "0 2108 4.165562 0.67985797\n",
      "0 2110 3.7377136 0.6798664\n",
      "0 2112 3.7184901 0.6798731\n",
      "0 2114 3.7759047 0.6798785\n",
      "0 2116 3.8222013 0.6798821\n",
      "0 2118 4.2636137 0.6798889\n",
      "0 2120 4.2172146 0.67989707\n",
      "0 2122 3.7324185 0.67990315\n",
      "0 2124 4.344819 0.6799125\n",
      "0 2126 4.2720118 0.6799245\n",
      "0 2128 4.243721 0.67993945\n",
      "0 2130 4.070643 0.67995393\n",
      "0 2132 4.507339 0.6799729\n",
      "0 2134 4.683458 0.6799961\n",
      "0 2136 3.8359056 0.68001795\n",
      "0 2138 3.9529355 0.68003714\n",
      "0 2140 3.8486943 0.68005455\n",
      "0 2142 3.6856923 0.6800687\n",
      "0 2144 4.026989 0.68008274\n",
      "0 2146 4.238761 0.680099\n",
      "0 2148 4.229791 0.6801165\n",
      "0 2150 4.155342 0.68013453\n",
      "0 2152 3.4298909 0.68014824\n",
      "0 2154 4.1270065 0.68016195\n",
      "0 2156 3.7954206 0.68017316\n",
      "0 2158 4.042336 0.6801834\n",
      "0 2160 4.076449 0.6801943\n",
      "0 2162 3.956756 0.68020505\n",
      "0 2164 3.94746 0.68021625\n",
      "0 2166 3.4548302 0.68022186\n",
      "0 2168 3.313025 0.6802231\n",
      "0 2170 3.9631114 0.68022406\n",
      "0 2172 3.6914299 0.6802231\n",
      "0 2174 4.0295615 0.68022496\n",
      "0 2176 3.7239358 0.6802258\n",
      "0 2178 4.284011 0.6802281\n",
      "0 2180 3.5960934 0.68022686\n",
      "0 2182 4.4884205 0.68023175\n",
      "0 2184 3.9631617 0.6802371\n",
      "0 2186 3.96461 0.6802426\n",
      "0 2188 3.995792 0.6802488\n",
      "0 2190 3.8047829 0.6802539\n",
      "0 2192 3.9727354 0.6802592\n",
      "0 2194 3.7288492 0.680262\n",
      "0 2196 4.009331 0.6802645\n",
      "0 2198 3.7509553 0.68026483\n",
      "0 2200 3.5441222 0.6802627\n",
      "0 2202 3.849298 0.6802608\n",
      "0 2204 4.180507 0.6802602\n",
      "0 2206 3.9917643 0.6802603\n",
      "0 2208 3.9302788 0.68026143\n",
      "0 2210 3.8233557 0.6802619\n",
      "0 2212 3.7791028 0.68026155\n",
      "0 2214 3.4770916 0.68025863\n",
      "0 2216 3.5200574 0.68025434\n",
      "0 2218 4.1085753 0.6802538\n",
      "0 2220 3.648995 0.6802487\n",
      "0 2222 3.7194834 0.6802426\n",
      "0 2224 4.2970734 0.6802402\n",
      "0 2226 3.8328342 0.6802373\n",
      "0 2228 4.072416 0.68023556\n",
      "0 2230 4.194276 0.6802368\n",
      "0 2232 4.27579 0.68024135\n",
      "0 2234 4.3583198 0.6802483\n",
      "0 2236 3.8500772 0.6802531\n",
      "0 2238 3.7433023 0.6802554\n",
      "0 2240 4.60296 0.680264\n",
      "0 2242 4.086056 0.6802729\n",
      "0 2244 3.8414602 0.68028027\n",
      "0 2246 4.1975107 0.68028927\n",
      "0 2248 4.000716 0.6802981\n",
      "0 2250 3.8698435 0.6803062\n",
      "0 2252 3.9820406 0.6803131\n",
      "0 2254 4.1963344 0.6803223\n",
      "0 2256 3.9622934 0.68033177\n",
      "0 2258 4.1150823 0.6803423\n",
      "0 2260 3.605576 0.68034905\n",
      "0 2262 4.1233315 0.68035626\n",
      "0 2264 4.0824866 0.6803643\n",
      "0 2266 3.9275324 0.6803718\n",
      "0 2268 3.6829977 0.6803761\n",
      "0 2270 3.7764914 0.6803788\n",
      "0 2272 4.0096893 0.6803816\n",
      "0 2274 3.7352366 0.680383\n",
      "0 2276 4.2157435 0.6803869\n",
      "0 2278 3.781829 0.6803886\n",
      "0 2280 3.618857 0.6803869\n",
      "0 2282 3.892495 0.6803833\n",
      "0 2284 4.208638 0.6803843\n",
      "0 2286 4.1114364 0.6803869\n",
      "0 2288 3.8038285 0.68038917\n",
      "0 2290 4.2552805 0.68039525\n",
      "0 2292 3.8872955 0.6804003\n",
      "0 2294 3.8783529 0.6804041\n",
      "0 2296 3.6580572 0.6804052\n",
      "0 2298 3.746753 0.68040264\n",
      "0 2300 3.4678981 0.6803993\n",
      "0 2302 4.1131763 0.68039775\n",
      "0 2304 4.067578 0.68039775\n",
      "0 2306 3.6879613 0.6803963\n",
      "0 2308 3.5350893 0.6803932\n",
      "0 2310 4.0067873 0.6803914\n",
      "0 2312 3.802171 0.6803897\n",
      "0 2314 3.8615913 0.68038756\n",
      "0 2316 4.080478 0.6803876\n",
      "0 2318 3.8392038 0.680388\n",
      "0 2320 4.0492334 0.6803888\n",
      "0 2322 3.685888 0.68038815\n",
      "0 2324 3.950972 0.68038833\n",
      "0 2326 4.005792 0.6803891\n",
      "0 2328 4.164104 0.68039197\n",
      "0 2330 3.8963046 0.6803953\n",
      "0 2332 4.172387 0.68040013\n",
      "0 2334 4.186561 0.68040705\n",
      "0 2336 3.619511 0.6804113\n",
      "0 2338 3.826221 0.6804148\n",
      "0 2340 3.8275337 0.6804161\n",
      "0 2342 3.8385684 0.6804171\n",
      "0 2344 3.943514 0.68041784\n",
      "0 2346 4.310072 0.6804227\n",
      "0 2348 4.0641556 0.68042797\n",
      "0 2350 3.7424102 0.68043137\n",
      "0 2352 3.7897265 0.6804349\n",
      "0 2354 4.183387 0.68044\n",
      "0 2356 4.0531073 0.68044424\n",
      "0 2358 3.8127134 0.68044686\n",
      "0 2360 3.921872 0.6804495\n",
      "0 2362 3.779121 0.6804512\n",
      "0 2364 4.069709 0.68045455\n",
      "0 2366 3.997643 0.6804579\n",
      "0 2368 4.025135 0.6804618\n",
      "0 2370 4.043041 0.68046755\n",
      "0 2372 4.3828506 0.6804774\n",
      "0 2374 4.1763053 0.68048906\n",
      "0 2376 4.0550375 0.68050224\n",
      "0 2378 3.6619518 0.6805124\n",
      "0 2380 4.0258675 0.6805213\n",
      "0 2382 4.0740967 0.68053085\n",
      "0 2384 3.9483585 0.6805398\n",
      "0 2386 4.1286006 0.6805502\n",
      "0 2388 4.248277 0.68056303\n",
      "0 2390 4.0954695 0.6805765\n",
      "0 2392 3.939472 0.6805894\n",
      "0 2394 3.8471084 0.680601\n",
      "0 2396 4.0469112 0.68061405\n",
      "0 2398 3.5843189 0.68062365\n",
      "0 2400 4.428032 0.6806368\n",
      "0 2402 3.6324463 0.6806475\n",
      "0 2404 3.91107 0.680658\n",
      "0 2406 4.217106 0.6806699\n",
      "0 2408 3.9350195 0.68068117\n",
      "0 2410 3.9770627 0.6806924\n",
      "0 2412 3.938339 0.6807026\n",
      "0 2414 4.063742 0.6807121\n",
      "0 2416 3.9043658 0.6807217\n",
      "0 2418 3.7324734 0.68072945\n",
      "0 2420 3.6961596 0.68073535\n",
      "0 2422 4.0466547 0.68074185\n",
      "0 2424 3.9369702 0.6807488\n",
      "0 2426 3.6360145 0.6807534\n",
      "0 2428 3.8645487 0.6807574\n",
      "0 2430 3.6854892 0.68076015\n",
      "0 2432 4.056245 0.6807642\n",
      "0 2434 3.7972012 0.6807688\n",
      "0 2436 4.090052 0.6807736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-2712cdfec7f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Dinput:0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mBatchInput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Dinput_1:0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mUnlabelledInput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'GInput:0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mNoise\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mOutput\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#print(val)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDiscTrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mGenTrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai rathan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai rathan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai rathan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai rathan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai rathan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sai rathan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r=labelToCode()\n",
    "for i in range(numberofepochs):\n",
    "    print(\"EPOCH\",i)\n",
    "    for j in range(0,len(LabelledImages)-1,2):\n",
    "        BatchInput=[]\n",
    "        BatchInput.append(resizeImage(cv2.imread(\"E:\\\\DIP\\\\VOCtrain\\\\VOCdevkit\\\\VOC2012\\\\JPEGImages\\\\\"+LabelledImages[j]+'jpg')[:,:,::-1]))\n",
    "        BatchInput.append(resizeImage(cv2.imread(\"E:\\\\DIP\\\\VOCtrain\\\\VOCdevkit\\\\VOC2012\\\\JPEGImages\\\\\"+LabelledImages[j+1]+'jpg')[:,:,::-1]))\n",
    "        BatchInput=np.array(BatchInput,dtype=np.float32)\n",
    "        Output=[]\n",
    "        Output.append(imageToMatrix(\"E:\\\\DIP\\\\VOCtrain\\\\VOCdevkit\\\\VOC2012\\\\SegmentationClass\\\\\"+LabelledImages[j]+'png',r))\n",
    "        Output.append(imageToMatrix(\"E:\\\\DIP\\\\VOCtrain\\\\VOCdevkit\\\\VOC2012\\\\SegmentationClass\\\\\"+LabelledImages[j+1]+'png',r))        \n",
    "        Output=np.array(Output,dtype=np.float32)\n",
    "        UnlabelledInput=[]\n",
    "        UnlabelledInput.append(resizeImage(cv2.imread(\"E:\\\\DIP\\\\VOCtrain\\\\VOCdevkit\\\\VOC2012\\\\JPEGImages\\\\\"+UnlabelledImages[np.random.randint(0,14000)]+'jpg')[:,:,::-1]))\n",
    "        UnlabelledInput.append(resizeImage(cv2.imread(\"E:\\\\DIP\\\\VOCtrain\\\\VOCdevkit\\\\VOC2012\\\\JPEGImages\\\\\"+UnlabelledImages[np.random.randint(0,14000)]+'jpg')[:,:,::-1]))\n",
    "        UnlabelledInput=np.array(UnlabelledInput,dtype=np.float32)\n",
    "        Noise=np.array([np.random.randn(100),np.random.randn(100)],dtype=np.float32)\n",
    "        #print(Noise.dtype)\n",
    "        val={'Dinput:0':BatchInput,'Dinput_1:0':UnlabelledInput,'GInput:0':Noise,y:Output}\n",
    "        #print(val)\n",
    "        _,loss1=sess.run([DiscTrain,dloss],feed_dict=val)\n",
    "        _,loss2=sess.run([GenTrain,gloss],feed_dict=val)\n",
    "        print(i,j,loss1,loss2)\n",
    "        #print(sess.run(dloss_3,feed_dict=val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.00549919,  0.03337722,  0.01730994, ...,  0.00427328,\n",
       "           0.01704031,  0.00097677],\n",
       "         [ 0.0005865 , -0.00700154,  0.00911332, ..., -0.00948222,\n",
       "           0.00955431,  0.03342534],\n",
       "         [-0.02283746,  0.0144736 ,  0.00388372, ..., -0.02760927,\n",
       "          -0.00458399, -0.02616526],\n",
       "         ...,\n",
       "         [-0.00557281, -0.00057682,  0.03321423, ..., -0.00298839,\n",
       "           0.03125343, -0.00105238],\n",
       "         [-0.00326239,  0.04375004,  0.01176238, ..., -0.0105947 ,\n",
       "           0.03197195, -0.01477838],\n",
       "         [ 0.01482033,  0.03792546, -0.01172991, ..., -0.00399324,\n",
       "           0.01354544,  0.00412283]],\n",
       "\n",
       "        [[-0.01179501,  0.04332247,  0.00641119, ..., -0.00263163,\n",
       "           0.01271267,  0.01368327],\n",
       "         [ 0.01604692,  0.01819888, -0.02839476, ...,  0.01422643,\n",
       "           0.01346793, -0.02895385],\n",
       "         [-0.0081243 ,  0.01863332,  0.03358532, ...,  0.02587189,\n",
       "           0.03678839,  0.0181729 ],\n",
       "         ...,\n",
       "         [ 0.00088675,  0.04629343,  0.00232378, ...,  0.0043852 ,\n",
       "          -0.01211442,  0.03793919],\n",
       "         [-0.00257614, -0.01358572,  0.02883624, ...,  0.01650329,\n",
       "           0.04422604, -0.00046307],\n",
       "         [-0.01722436, -0.01003427, -0.00507669, ...,  0.00353287,\n",
       "          -0.00220468, -0.01859498]],\n",
       "\n",
       "        [[-0.0062678 ,  0.04670929,  0.00607586, ..., -0.01281251,\n",
       "          -0.01657381,  0.02371017],\n",
       "         [ 0.00765907, -0.00515516,  0.02552713, ...,  0.00128464,\n",
       "           0.02686912,  0.00651387],\n",
       "         [-0.01034756,  0.00032624,  0.04840339, ...,  0.00111855,\n",
       "           0.01739242, -0.00276151],\n",
       "         ...,\n",
       "         [ 0.00715824, -0.00020554,  0.01757451, ..., -0.00288412,\n",
       "           0.01220383,  0.00489274],\n",
       "         [ 0.03591221,  0.03757682,  0.00894888, ...,  0.03411478,\n",
       "           0.01261595,  0.03831692],\n",
       "         [ 0.00769952, -0.01350727,  0.01195658, ...,  0.02336788,\n",
       "           0.0118132 ,  0.02377012]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00156843,  0.00154599,  0.00110662, ...,  0.01727376,\n",
       "           0.00406916,  0.02485694],\n",
       "         [-0.00168624,  0.00991149, -0.01545952, ..., -0.0436803 ,\n",
       "           0.01029829, -0.00626753],\n",
       "         [ 0.03453788, -0.00280125,  0.00035264, ...,  0.00777973,\n",
       "           0.02465149, -0.01123565],\n",
       "         ...,\n",
       "         [ 0.03465877,  0.00949762,  0.04176997, ...,  0.00182234,\n",
       "           0.04151057,  0.01805629],\n",
       "         [ 0.01552937, -0.00165416,  0.02958801, ...,  0.0176504 ,\n",
       "           0.02649441,  0.01450269],\n",
       "         [ 0.00421022,  0.03300272,  0.04207942, ...,  0.01546821,\n",
       "           0.03466377,  0.02639962]],\n",
       "\n",
       "        [[ 0.0264705 ,  0.01263974, -0.04270001, ...,  0.00274373,\n",
       "           0.0118489 , -0.005743  ],\n",
       "         [ 0.0060422 , -0.00735153,  0.01505127, ...,  0.00629131,\n",
       "           0.0346692 ,  0.03344421],\n",
       "         [-0.01209565,  0.01243241,  0.04517753, ...,  0.04547272,\n",
       "          -0.01516494,  0.04247198],\n",
       "         ...,\n",
       "         [ 0.03677673,  0.02960259, -0.00085652, ..., -0.00624769,\n",
       "           0.03735949,  0.01485048],\n",
       "         [-0.00132834,  0.01720394,  0.01845403, ..., -0.00362818,\n",
       "           0.044848  ,  0.0385911 ],\n",
       "         [-0.00922954, -0.00611983, -0.01891736, ...,  0.01807768,\n",
       "           0.01173169, -0.02099806]],\n",
       "\n",
       "        [[ 0.01034534,  0.00477479,  0.00496339, ...,  0.03632814,\n",
       "           0.02748974,  0.0418793 ],\n",
       "         [ 0.01798359,  0.03710214, -0.00479049, ...,  0.02284267,\n",
       "          -0.00554836, -0.00501423],\n",
       "         [-0.00678238,  0.01147794,  0.02261079, ...,  0.04733429,\n",
       "           0.01229285, -0.00715916],\n",
       "         ...,\n",
       "         [ 0.03288243,  0.02830603,  0.03798479, ...,  0.03377981,\n",
       "          -0.01351455, -0.00617198],\n",
       "         [ 0.03006177,  0.02093041,  0.00631594, ...,  0.01607468,\n",
       "           0.01133514,  0.01342624],\n",
       "         [ 0.00320899, -0.02195259, -0.01934397, ..., -0.00855337,\n",
       "           0.00543078,  0.02388551]]],\n",
       "\n",
       "\n",
       "       [[[-0.00916969, -0.00910817,  0.02689727, ...,  0.04023631,\n",
       "          -0.01320629,  0.04470932],\n",
       "         [ 0.03324695,  0.03491861,  0.03566777, ...,  0.02691579,\n",
       "           0.01739074,  0.04648577],\n",
       "         [ 0.00935313,  0.03970914, -0.00212382, ...,  0.03330713,\n",
       "           0.01656154,  0.02719733],\n",
       "         ...,\n",
       "         [-0.01103227,  0.02351549,  0.04308429, ...,  0.02474064,\n",
       "          -0.01611021,  0.01876972],\n",
       "         [ 0.01083246, -0.01052106,  0.01385999, ...,  0.02705242,\n",
       "           0.03349134,  0.03113848],\n",
       "         [ 0.02479991, -0.01716318,  0.00451123, ..., -0.01592065,\n",
       "           0.01402598,  0.02690508]],\n",
       "\n",
       "        [[ 0.03233726, -0.00786846, -0.01480925, ...,  0.01135342,\n",
       "           0.04233225,  0.01695991],\n",
       "         [ 0.04318547,  0.0338674 ,  0.0055959 , ...,  0.02187006,\n",
       "           0.01880963,  0.00270678],\n",
       "         [ 0.04352738,  0.00261463,  0.04693182, ..., -0.00052649,\n",
       "          -0.01215017, -0.00123917],\n",
       "         ...,\n",
       "         [ 0.01954698, -0.01456168, -0.02558764, ..., -0.01077323,\n",
       "          -0.0101609 , -0.03804814],\n",
       "         [-0.00315822,  0.03848419,  0.02454192, ...,  0.02810917,\n",
       "           0.03632232,  0.00323193],\n",
       "         [-0.00310872,  0.01475361,  0.00018137, ..., -0.00779165,\n",
       "           0.03070552,  0.02907142]],\n",
       "\n",
       "        [[ 0.00818383,  0.03644157,  0.02613552, ...,  0.03958369,\n",
       "           0.04263515,  0.00028983],\n",
       "         [ 0.03922429,  0.01179337, -0.00299636, ...,  0.01727312,\n",
       "          -0.01044076,  0.00166145],\n",
       "         [ 0.04216735,  0.00937064,  0.02838333, ...,  0.02366064,\n",
       "           0.01648512,  0.02906709],\n",
       "         ...,\n",
       "         [-0.01331379,  0.00167203,  0.00092709, ...,  0.01596608,\n",
       "          -0.00586468,  0.01000304],\n",
       "         [ 0.01521152,  0.02814774,  0.00707439, ...,  0.00111209,\n",
       "          -0.01084681, -0.00828051],\n",
       "         [ 0.00126433,  0.00377471,  0.0012571 , ...,  0.04018661,\n",
       "          -0.02222797,  0.01751463]]]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.get_default_graph().get_tensor_by_name('DW14:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 4.07882966e-04,  4.82621184e-03, -9.11695044e-03, ...,\n",
       "           1.51569908e-02, -2.73966836e-03, -4.83311387e-03],\n",
       "         [ 3.48924054e-03,  2.60589737e-03, -8.10198858e-03, ...,\n",
       "           8.86987546e-04, -7.08238874e-03, -7.96321197e-04],\n",
       "         [-1.12759033e-02, -6.43836753e-03, -8.05236772e-03, ...,\n",
       "           1.90410519e-03,  3.87627119e-03,  1.96338408e-02],\n",
       "         ...,\n",
       "         [-1.49949323e-02,  1.20318262e-02,  9.87236737e-04, ...,\n",
       "          -2.89861043e-03,  4.03315574e-03, -5.93429338e-03],\n",
       "         [-7.53336679e-03,  6.08530594e-03,  9.90014290e-04, ...,\n",
       "           1.63243050e-04, -3.09621776e-03, -3.01808352e-03],\n",
       "         [-2.62522907e-03,  3.51887429e-03, -2.58309790e-03, ...,\n",
       "          -6.02027494e-03, -8.85035843e-03,  9.85825085e-04]],\n",
       "\n",
       "        [[ 3.02846078e-04,  2.64736195e-03, -1.37099335e-02, ...,\n",
       "           1.49135189e-02, -4.65666235e-04, -6.84898719e-03],\n",
       "         [-9.60117579e-03,  5.01031429e-03, -1.13053191e-02, ...,\n",
       "           8.86707660e-03,  6.21224102e-03,  2.48882570e-03],\n",
       "         [-1.13499342e-02, -4.72285505e-03, -8.57903156e-03, ...,\n",
       "          -3.04920389e-03,  9.67197306e-03,  1.90250184e-02],\n",
       "         ...,\n",
       "         [-4.48127137e-03,  5.53716440e-03,  4.68912302e-03, ...,\n",
       "          -4.91005788e-03,  6.40070112e-03, -8.28019064e-03],\n",
       "         [-6.00830279e-03, -7.51605199e-04,  8.29616503e-04, ...,\n",
       "           2.07461347e-03,  3.22994636e-03, -1.69727230e-03],\n",
       "         [-2.81035155e-03,  1.36994272e-02, -2.04460253e-03, ...,\n",
       "          -5.17683988e-03, -9.04289633e-03, -1.79657899e-03]],\n",
       "\n",
       "        [[ 3.09069827e-03, -7.85107608e-04, -1.12262042e-02, ...,\n",
       "           1.38624888e-02, -1.50248851e-03, -2.83075101e-03],\n",
       "         [-1.78263709e-02, -4.06336552e-03, -1.11764381e-02, ...,\n",
       "           1.76884588e-02, -1.14172359e-03,  1.50952139e-03],\n",
       "         [-1.26420632e-02, -1.19902752e-03, -1.03873471e-02, ...,\n",
       "          -1.54873531e-03,  3.21487361e-03,  1.84067711e-02],\n",
       "         ...,\n",
       "         [ 4.99952445e-03,  8.96072667e-03,  2.35912274e-03, ...,\n",
       "          -6.95799524e-03,  9.11425054e-03, -1.76598097e-03],\n",
       "         [-3.11188446e-03, -4.81695170e-03,  2.14671413e-03, ...,\n",
       "           8.69878661e-03,  4.66441829e-03,  3.96351190e-03],\n",
       "         [-1.56560540e-03,  1.01758400e-02, -1.81451999e-03, ...,\n",
       "          -7.93038029e-03, -7.92690739e-03,  9.85421613e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 2.21009902e-03,  5.53841889e-03, -1.51329748e-02, ...,\n",
       "           1.92244705e-02, -2.94085429e-03, -2.05214042e-03],\n",
       "         [ 4.31180233e-03, -4.67703957e-03, -9.83539224e-03, ...,\n",
       "          -1.16664264e-03, -7.47026131e-03,  8.98758229e-03],\n",
       "         [-1.00150378e-02, -3.31197702e-03, -4.38165246e-03, ...,\n",
       "           1.28704909e-04,  6.90069282e-05,  5.32614533e-03],\n",
       "         ...,\n",
       "         [-1.65325180e-02,  5.79520175e-03, -1.08136714e-03, ...,\n",
       "          -3.87264206e-03,  3.87065555e-03, -8.52444116e-03],\n",
       "         [-1.10835498e-02,  6.04208373e-03,  6.12878706e-03, ...,\n",
       "          -4.66409931e-03, -1.65367487e-03, -4.93706390e-03],\n",
       "         [-5.82469627e-03,  2.77068373e-03,  2.59765168e-03, ...,\n",
       "          -9.57501866e-03, -3.32338479e-03,  3.07677989e-03]],\n",
       "\n",
       "        [[ 3.59932799e-03,  1.19693214e-02, -1.60994809e-02, ...,\n",
       "           1.80910621e-02,  3.29896147e-06, -6.10836258e-04],\n",
       "         [-7.41903298e-03, -5.61146066e-03, -1.38216335e-02, ...,\n",
       "           2.96197180e-03,  2.69207382e-03,  7.01817824e-03],\n",
       "         [-8.48251395e-03,  9.95878596e-04,  6.25522574e-04, ...,\n",
       "          -3.68017773e-03,  1.27349654e-02,  4.87677474e-03],\n",
       "         ...,\n",
       "         [-8.20725970e-03,  1.95525214e-03,  2.69707199e-03, ...,\n",
       "          -4.49266564e-03,  6.45229314e-03, -1.28380703e-02],\n",
       "         [-5.50353806e-03,  1.61080994e-03,  1.39699667e-03, ...,\n",
       "          -2.52319360e-03, -4.11227718e-03, -5.21757640e-03],\n",
       "         [-2.69698747e-03,  2.70054420e-03,  3.79564380e-03, ...,\n",
       "          -7.66708190e-03, -9.20492224e-03, -4.52092383e-03]],\n",
       "\n",
       "        [[ 2.48262100e-03,  6.13112841e-03, -1.47995697e-02, ...,\n",
       "           1.69335436e-02,  1.16662926e-03,  1.17623655e-03],\n",
       "         [-1.69411730e-02, -8.71018134e-03, -1.29530905e-02, ...,\n",
       "           1.87354535e-03, -6.88433880e-04, -1.68701622e-03],\n",
       "         [-6.59902534e-03,  3.46401357e-03, -3.11806914e-03, ...,\n",
       "          -4.05590283e-03,  7.37769250e-03,  5.51742781e-03],\n",
       "         ...,\n",
       "         [ 9.51702707e-04,  1.64372730e-03,  1.49127271e-03, ...,\n",
       "          -7.65326899e-03,  1.19846454e-02, -9.10102390e-03],\n",
       "         [ 2.88849953e-03, -5.35335718e-03, -3.08376068e-04, ...,\n",
       "           3.87483276e-04, -4.36716061e-03, -2.73425644e-03],\n",
       "         [-1.54271268e-03,  4.30263951e-03, -4.31923472e-05, ...,\n",
       "          -5.21167647e-03, -8.79874174e-03, -2.11346560e-04]]],\n",
       "\n",
       "\n",
       "       [[[-2.93453527e-03,  4.62880917e-03, -9.69677325e-03, ...,\n",
       "           1.48544796e-02,  3.87839018e-03,  1.85810984e-03],\n",
       "         [ 5.63784037e-03, -6.04889123e-03, -5.59245935e-03, ...,\n",
       "          -4.98529337e-03, -1.92062720e-03,  4.07473044e-03],\n",
       "         [-4.84194839e-03,  2.70295335e-04, -3.02166585e-03, ...,\n",
       "          -1.49492722e-03, -1.31164724e-02,  2.69274553e-03],\n",
       "         ...,\n",
       "         [-1.79702230e-02, -3.29879066e-03, -6.12194091e-03, ...,\n",
       "          -7.89267663e-03, -1.74841529e-03, -2.64002383e-03],\n",
       "         [-1.11466460e-02,  5.13884844e-03,  1.51880039e-02, ...,\n",
       "          -4.87166690e-03,  2.45198980e-03, -3.42866592e-03],\n",
       "         [-6.12380216e-03,  8.87359586e-03, -3.45014478e-03, ...,\n",
       "          -1.09723555e-02,  2.66773975e-03, -4.79325093e-03]],\n",
       "\n",
       "        [[-6.14633691e-03,  1.40388934e-02, -9.19489283e-03, ...,\n",
       "           1.60656814e-02,  8.88941437e-03,  6.56510517e-03],\n",
       "         [-5.72578516e-03, -3.33702704e-03, -8.64025764e-03, ...,\n",
       "           1.98416272e-03,  7.86264334e-03,  2.50778953e-03],\n",
       "         [-3.77366156e-03,  9.24240507e-04,  7.29091058e-04, ...,\n",
       "          -4.96890210e-03,  3.85057204e-03, -7.15580711e-04],\n",
       "         ...,\n",
       "         [-1.22111076e-02, -2.42003356e-03, -4.81141545e-03, ...,\n",
       "          -4.22020489e-03, -1.01946108e-02, -5.80721162e-03],\n",
       "         [-9.04400460e-03,  1.94370467e-03,  7.34009594e-03, ...,\n",
       "          -5.14973374e-03,  3.66879918e-04, -5.37241250e-03],\n",
       "         [-3.53684562e-04,  1.53788179e-02, -8.76262202e-04, ...,\n",
       "          -7.13666994e-03, -4.27656015e-03, -1.42133404e-02]],\n",
       "\n",
       "        [[-6.67408668e-03,  9.11168288e-03, -9.18486901e-03, ...,\n",
       "           1.21346870e-02,  8.51947255e-03,  7.15135690e-03],\n",
       "         [-1.64910071e-02, -2.04550894e-03, -7.30760070e-03, ...,\n",
       "          -2.89353775e-03,  1.16095145e-03, -6.60440046e-03],\n",
       "         [-7.68517129e-05,  4.32093628e-03, -6.93119306e-04, ...,\n",
       "          -3.48316878e-03, -1.47545757e-03, -2.85687298e-03],\n",
       "         ...,\n",
       "         [-4.66702832e-03, -1.39780261e-03,  3.67210196e-05, ...,\n",
       "          -4.95146902e-04,  7.23151665e-04, -6.29519764e-03],\n",
       "         [ 3.63699184e-03, -4.63909749e-03,  5.31478412e-03, ...,\n",
       "          -1.50180201e-03, -7.48660788e-03, -1.94359326e-03],\n",
       "         [-1.03440508e-03,  1.57131702e-02, -1.98940560e-03, ...,\n",
       "          -5.38550608e-04, -5.37431426e-03, -9.20103770e-03]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['conv5_3_W']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 0 4.5219398 0.6931415\n",
    "0 2 4.521087 0.6931358\n",
    "0 4 4.521656 0.6931305\n",
    "0 6 4.5212517 0.69312507\n",
    "0 8 4.5208836 0.6931195\n",
    "0 10 4.5215898 0.6931138"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
